[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:485]: MegatronTrainer: monkey patch TopKRouter...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:356]: MegatronTrainer: monkey patch get_extra_te_kwargs...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:588]: MegatronTrainer: Patching FileSystemWriterAsync...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:600]: MegatronTrainer: Patch FileSystemWriterAsync successfully.
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:282]: MegatronTrainer: Patch get_fp8_context...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:694] : -run update_primus_config...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:810] : -rank:              0
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:811] : -local_rank:        0
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:812] : -world_size:        8
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:829] : -save:              /workspace/Primus/output/amd/root/qwen2.5_7B-pretrain/checkpoints
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:836] : -auto_continue_train:0
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:872] : -disable_tensorboard:True
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:873] :   -tensorboard_dir: None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:890] : -disable_wandb:     True
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:896] :   -wandb_project:   None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:897] :   -wandb_exp_name:  None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:898] :   -wandb_save_dir:  None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:899] :   -wandb_entity:    None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:902] : -disable_mlflow:    True
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:916] :   -mlflow_run_name: None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:917] :   -mlflow_experiment_name:None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [-----import_utils.py:30] : [Primus][MegatronCompat] Loaded model_provider from model_provider
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [-----import_utils.py:30] : [Primus][MegatronCompat] Loaded gpt_builder from gpt_builders
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:702] : -run initialize_megatron...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1158] : -load:              None
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1159] : -use_checkpoint_args:False
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1185] : -monkey patch megatron.training.global_vars._set_wandb_writer...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1190] : -set_global_variables...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1192] : -set_primus_global_variables...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1197] : -build_tokenizer...
[20251218 08:35:25][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------tokenizer.py:40] : -building HuggingFaceTokenizer tokenizer...
[20251218 08:35:26][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1250] : -lazy_mpu_init:     None
[20251218 08:35:26][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1227] : -initialize_distributed...
[20251218 08:35:26][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1231] : -seeds:             1234
[20251218 08:35:36][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:761] : time to initialize megatron (seconds): 12.249
[20251218 08:35:36][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [---------trainer.py:994] : -setup_model_and_optimizer...
[20251218 08:35:36][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [trainer.py:243]: MegatronTrainer: patch TESpecProvider to PrimusTurboSpecProvider, `enable_primus_turbo=True` will use PrimusTurbo backend
[20251218 08:35:36][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1299] : use pt backend...
[20251218 08:35:36][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1307] : -run get_model
[20251218 08:35:37][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1309] : [DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-27): 28 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): PrimusTurboAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear(in_features=3584, out_features=3584, bias=False, TP=1)
              (linear_qkv): TELayerNormColumnParallelLinear(in_features=3584, out_features=4608, bias=False, TP=1)
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): IdentityOp()
            (mlp): MLP(
              (linear_fc1): TELayerNormColumnParallelLinear(in_features=3584, out_features=37888, bias=False, TP=1)
              (linear_fc2): TERowParallelLinear(in_features=18944, out_features=3584, bias=False, TP=1)
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear(in_features=3584, out_features=151680, bias=False, TP=1)
    )
  )
)]
[20251218 08:35:37][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1323] : -run get_megatron_optimizer
[20251218 08:35:37][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1111] : > building train, validation, and test datasets for GPT ...
[20251218 08:35:37][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1119] : > finished creating GPT datasets ...
[20251218 08:35:37][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1058] : done with setup ...
[20251218 08:35:38][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1453] : training ...
[20251218 08:35:38][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [--------trainer.py:1597] : Setting rerun_state_machine.current_iteration to 0...
[20251218 08:35:38][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][INFO ] [-----import_utils.py:30] : [Primus][MegatronCompat] Loaded FullyShardedDataParallel from megatron.core.distributed.fsdp.mcore_fsdp_adapter
[20251218 08:35:40][root/amd][pre_trainer][ip-10.21.9.25][rank-0/8][WARNING] [fp8_utils.py:156]: WARNING: Primus-Turbo FP8 delayed not work since Primus-Turbo not support delayed scaling..
