[20251218 08:15:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        1/      50 | consumed samples:          256 | elapsed time per iteration (ms): 53350.2/53350.2 | hip mem usage/free/total/usage_ratio: 152.14GB/39.85GB/191.98GB/79.24% | rocm mem usage/free/total/usage_ratio: 152.34GB/39.64GB/191.98GB/79.35% | throughput per GPU (TFLOP/s/GPU): 284.6/284.6 | tokens per GPU (tokens/s/GPU): 4913.6/4913.6 | learning rate: 5.000000E-06 | global batch size:   256 | lm loss: 1.189806E+01 | loss scale: 1.0 | grad norm: 2.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:16:13][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        2/      50 | consumed samples:          512 | elapsed time per iteration (ms): 22465.6/37907.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | rocm mem usage/free/total/usage_ratio: 160.17GB/31.81GB/191.98GB/83.43% | throughput per GPU (TFLOP/s/GPU): 675.8/480.2 | tokens per GPU (tokens/s/GPU): 11668.7/8291.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.189686E+01 | loss scale: 1.0 | grad norm: 2.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:16:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        3/      50 | consumed samples:          768 | elapsed time per iteration (ms): 22445.6/22445.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 676.4/676.4 | tokens per GPU (tokens/s/GPU): 11679.1/11679.1 | learning rate: 9.989295E-06 | global batch size:   256 | lm loss: 1.153998E+01 | loss scale: 1.0 | grad norm: 2.990 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:16:58][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        4/      50 | consumed samples:         1024 | elapsed time per iteration (ms): 22506.8/22476.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.5/675.5 | tokens per GPU (tokens/s/GPU): 11647.3/11663.2 | learning rate: 9.957224E-06 | global batch size:   256 | lm loss: 1.096591E+01 | loss scale: 1.0 | grad norm: 3.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:17:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        5/      50 | consumed samples:         1280 | elapsed time per iteration (ms): 22509.7/22487.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.4/675.1 | tokens per GPU (tokens/s/GPU): 11645.8/11657.4 | learning rate: 9.903926E-06 | global batch size:   256 | lm loss: 1.055143E+01 | loss scale: 1.0 | grad norm: 4.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:17:43][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        6/      50 | consumed samples:         1536 | elapsed time per iteration (ms): 22487.2/22487.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.1/675.1 | tokens per GPU (tokens/s/GPU): 11657.5/11657.4 | learning rate: 9.829630E-06 | global batch size:   256 | lm loss: 1.019356E+01 | loss scale: 1.0 | grad norm: 6.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:18:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        7/      50 | consumed samples:         1792 | elapsed time per iteration (ms): 22461.9/22482.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.9/675.3 | tokens per GPU (tokens/s/GPU): 11670.6/11660.1 | learning rate: 9.734651E-06 | global batch size:   256 | lm loss: 9.687737E+00 | loss scale: 1.0 | grad norm: 9.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:18:28][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        8/      50 | consumed samples:         2048 | elapsed time per iteration (ms): 22473.3/22480.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.3 | tokens per GPU (tokens/s/GPU): 11664.7/11660.8 | learning rate: 9.619398E-06 | global batch size:   256 | lm loss: 9.164751E+00 | loss scale: 1.0 | grad norm: 9.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:18:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration        9/      50 | consumed samples:         2304 | elapsed time per iteration (ms): 22472.6/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.3 | tokens per GPU (tokens/s/GPU): 11665.1/11661.4 | learning rate: 9.484364E-06 | global batch size:   256 | lm loss: 8.593085E+00 | loss scale: 1.0 | grad norm: 8.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:19:13][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       10/      50 | consumed samples:         2560 | elapsed time per iteration (ms): 22478.5/22479.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.4 | tokens per GPU (tokens/s/GPU): 11662.0/11661.5 | learning rate: 9.330127E-06 | global batch size:   256 | lm loss: 8.169315E+00 | loss scale: 1.0 | grad norm: 8.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:19:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       11/      50 | consumed samples:         2816 | elapsed time per iteration (ms): 22479.8/22479.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.4 | tokens per GPU (tokens/s/GPU): 11661.3/11661.5 | learning rate: 9.157348E-06 | global batch size:   256 | lm loss: 7.780855E+00 | loss scale: 1.0 | grad norm: 8.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:19:58][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       12/      50 | consumed samples:         3072 | elapsed time per iteration (ms): 22480.5/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11661.0/11661.4 | learning rate: 8.966766E-06 | global batch size:   256 | lm loss: 7.481033E+00 | loss scale: 1.0 | grad norm: 8.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:20:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       13/      50 | consumed samples:         3328 | elapsed time per iteration (ms): 22482.7/22479.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11659.8/11661.3 | learning rate: 8.759199E-06 | global batch size:   256 | lm loss: 7.169314E+00 | loss scale: 1.0 | grad norm: 10.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:20:42][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       14/      50 | consumed samples:         3584 | elapsed time per iteration (ms): 22474.0/22479.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.4 | tokens per GPU (tokens/s/GPU): 11664.3/11661.5 | learning rate: 8.535534E-06 | global batch size:   256 | lm loss: 6.815296E+00 | loss scale: 1.0 | grad norm: 10.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:21:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       15/      50 | consumed samples:         3840 | elapsed time per iteration (ms): 22482.0/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11660.2/11661.4 | learning rate: 8.296729E-06 | global batch size:   256 | lm loss: 6.555819E+00 | loss scale: 1.0 | grad norm: 11.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:21:27][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       16/      50 | consumed samples:         4096 | elapsed time per iteration (ms): 22481.4/22479.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11660.5/11661.4 | learning rate: 8.043807E-06 | global batch size:   256 | lm loss: 6.376563E+00 | loss scale: 1.0 | grad norm: 13.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:21:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       17/      50 | consumed samples:         4352 | elapsed time per iteration (ms): 22470.7/22479.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.4 | tokens per GPU (tokens/s/GPU): 11666.0/11661.7 | learning rate: 7.777851E-06 | global batch size:   256 | lm loss: 6.151232E+00 | loss scale: 1.0 | grad norm: 14.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:22:12][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       18/      50 | consumed samples:         4608 | elapsed time per iteration (ms): 22477.3/22479.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.4 | tokens per GPU (tokens/s/GPU): 11662.6/11661.7 | learning rate: 7.500000E-06 | global batch size:   256 | lm loss: 6.045423E+00 | loss scale: 1.0 | grad norm: 15.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:22:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       19/      50 | consumed samples:         4864 | elapsed time per iteration (ms): 22495.0/22479.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.9/675.3 | tokens per GPU (tokens/s/GPU): 11653.4/11661.2 | learning rate: 7.211444E-06 | global batch size:   256 | lm loss: 5.881318E+00 | loss scale: 1.0 | grad norm: 15.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:22:57][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       20/      50 | consumed samples:         5120 | elapsed time per iteration (ms): 22483.9/22480.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.3 | tokens per GPU (tokens/s/GPU): 11659.2/11661.1 | learning rate: 6.913417E-06 | global batch size:   256 | lm loss: 5.755231E+00 | loss scale: 1.0 | grad norm: 18.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:23:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       21/      50 | consumed samples:         5376 | elapsed time per iteration (ms): 22483.5/22480.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.3 | tokens per GPU (tokens/s/GPU): 11659.4/11661.0 | learning rate: 6.607198E-06 | global batch size:   256 | lm loss: 5.650851E+00 | loss scale: 1.0 | grad norm: 18.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:23:42][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       22/      50 | consumed samples:         5632 | elapsed time per iteration (ms): 22477.2/22480.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.3 | tokens per GPU (tokens/s/GPU): 11662.7/11661.1 | learning rate: 6.294095E-06 | global batch size:   256 | lm loss: 5.612560E+00 | loss scale: 1.0 | grad norm: 20.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:24:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       23/      50 | consumed samples:         5888 | elapsed time per iteration (ms): 22470.3/22479.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.3 | tokens per GPU (tokens/s/GPU): 11666.2/11661.4 | learning rate: 5.975452E-06 | global batch size:   256 | lm loss: 5.469757E+00 | loss scale: 1.0 | grad norm: 22.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:24:27][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       24/      50 | consumed samples:         6144 | elapsed time per iteration (ms): 22464.8/22479.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.8/675.4 | tokens per GPU (tokens/s/GPU): 11669.1/11661.7 | learning rate: 5.652631E-06 | global batch size:   256 | lm loss: 5.307073E+00 | loss scale: 1.0 | grad norm: 24.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:24:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       25/      50 | consumed samples:         6400 | elapsed time per iteration (ms): 22447.6/22477.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 676.3/675.4 | tokens per GPU (tokens/s/GPU): 11678.1/11662.4 | learning rate: 5.327016E-06 | global batch size:   256 | lm loss: 5.141537E+00 | loss scale: 1.0 | grad norm: 26.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:25:12][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       26/      50 | consumed samples:         6656 | elapsed time per iteration (ms): 22474.9/22477.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.4 | tokens per GPU (tokens/s/GPU): 11663.9/11662.5 | learning rate: 5.000000E-06 | global batch size:   256 | lm loss: 5.054605E+00 | loss scale: 1.0 | grad norm: 26.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:25:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       27/      50 | consumed samples:         6912 | elapsed time per iteration (ms): 23003.1/22498.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 660.0/674.8 | tokens per GPU (tokens/s/GPU): 11396.0/11651.8 | learning rate: 4.672984E-06 | global batch size:   256 | lm loss: 4.834011E+00 | loss scale: 1.0 | grad norm: 27.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:25:58][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       28/      50 | consumed samples:         7168 | elapsed time per iteration (ms): 22471.9/22497.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/674.8 | tokens per GPU (tokens/s/GPU): 11665.4/11652.4 | learning rate: 4.347369E-06 | global batch size:   256 | lm loss: 4.673080E+00 | loss scale: 1.0 | grad norm: 27.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:26:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       29/      50 | consumed samples:         7424 | elapsed time per iteration (ms): 22468.6/22496.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.7/674.9 | tokens per GPU (tokens/s/GPU): 11667.1/11652.9 | learning rate: 4.024549E-06 | global batch size:   256 | lm loss: 4.520215E+00 | loss scale: 1.0 | grad norm: 30.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:26:43][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       30/      50 | consumed samples:         7680 | elapsed time per iteration (ms): 22463.3/22495.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.8/674.9 | tokens per GPU (tokens/s/GPU): 11669.9/11653.5 | learning rate: 3.705905E-06 | global batch size:   256 | lm loss: 4.383165E+00 | loss scale: 1.0 | grad norm: 29.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:27:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       31/      50 | consumed samples:         7936 | elapsed time per iteration (ms): 22470.2/22494.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/674.9 | tokens per GPU (tokens/s/GPU): 11666.3/11653.9 | learning rate: 3.392803E-06 | global batch size:   256 | lm loss: 4.145537E+00 | loss scale: 1.0 | grad norm: 30.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:27:28][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       32/      50 | consumed samples:         8192 | elapsed time per iteration (ms): 22467.0/22493.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.7/674.9 | tokens per GPU (tokens/s/GPU): 11667.9/11654.4 | learning rate: 3.086583E-06 | global batch size:   256 | lm loss: 4.018104E+00 | loss scale: 1.0 | grad norm: 28.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:27:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       33/      50 | consumed samples:         8448 | elapsed time per iteration (ms): 22472.8/22492.8 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.0 | tokens per GPU (tokens/s/GPU): 11664.9/11654.8 | learning rate: 2.788556E-06 | global batch size:   256 | lm loss: 3.885016E+00 | loss scale: 1.0 | grad norm: 27.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:28:12][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       34/      50 | consumed samples:         8704 | elapsed time per iteration (ms): 22478.6/22492.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.0 | tokens per GPU (tokens/s/GPU): 11661.9/11655.0 | learning rate: 2.500000E-06 | global batch size:   256 | lm loss: 3.726072E+00 | loss scale: 1.0 | grad norm: 23.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:28:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       35/      50 | consumed samples:         8960 | elapsed time per iteration (ms): 22484.9/22492.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.0 | tokens per GPU (tokens/s/GPU): 11658.7/11655.1 | learning rate: 2.222149E-06 | global batch size:   256 | lm loss: 3.581526E+00 | loss scale: 1.0 | grad norm: 23.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:28:57][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       36/      50 | consumed samples:         9216 | elapsed time per iteration (ms): 22500.3/22492.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.7/675.0 | tokens per GPU (tokens/s/GPU): 11650.7/11655.0 | learning rate: 1.956193E-06 | global batch size:   256 | lm loss: 3.492316E+00 | loss scale: 1.0 | grad norm: 21.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:29:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       37/      50 | consumed samples:         9472 | elapsed time per iteration (ms): 22499.4/22492.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.8/675.0 | tokens per GPU (tokens/s/GPU): 11651.1/11654.8 | learning rate: 1.703271E-06 | global batch size:   256 | lm loss: 3.318976E+00 | loss scale: 1.0 | grad norm: 20.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:29:42][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       38/      50 | consumed samples:         9728 | elapsed time per iteration (ms): 22505.8/22493.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.6/675.0 | tokens per GPU (tokens/s/GPU): 11647.9/11654.7 | learning rate: 1.464466E-06 | global batch size:   256 | lm loss: 3.240969E+00 | loss scale: 1.0 | grad norm: 19.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:30:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       39/      50 | consumed samples:         9984 | elapsed time per iteration (ms): 22498.1/22493.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.8/674.9 | tokens per GPU (tokens/s/GPU): 11651.8/11654.6 | learning rate: 1.240801E-06 | global batch size:   256 | lm loss: 3.152150E+00 | loss scale: 1.0 | grad norm: 17.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:30:27][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       40/      50 | consumed samples:        10240 | elapsed time per iteration (ms): 22500.3/22493.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.7/674.9 | tokens per GPU (tokens/s/GPU): 11650.7/11654.5 | learning rate: 1.033233E-06 | global batch size:   256 | lm loss: 2.999306E+00 | loss scale: 1.0 | grad norm: 16.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:30:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       41/      50 | consumed samples:        10496 | elapsed time per iteration (ms): 22516.0/22493.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.3/674.9 | tokens per GPU (tokens/s/GPU): 11642.6/11654.2 | learning rate: 8.426520E-07 | global batch size:   256 | lm loss: 3.024853E+00 | loss scale: 1.0 | grad norm: 16.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:31:13][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       42/      50 | consumed samples:        10752 | elapsed time per iteration (ms): 22523.2/22494.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.9 | tokens per GPU (tokens/s/GPU): 11638.8/11653.8 | learning rate: 6.698730E-07 | global batch size:   256 | lm loss: 2.928720E+00 | loss scale: 1.0 | grad norm: 16.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:31:35][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       43/      50 | consumed samples:        11008 | elapsed time per iteration (ms): 22533.5/22495.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.9 | tokens per GPU (tokens/s/GPU): 11633.5/11653.3 | learning rate: 5.156363E-07 | global batch size:   256 | lm loss: 2.889323E+00 | loss scale: 1.0 | grad norm: 14.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:31:58][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       44/      50 | consumed samples:        11264 | elapsed time per iteration (ms): 22524.9/22496.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.9 | tokens per GPU (tokens/s/GPU): 11638.0/11652.9 | learning rate: 3.806023E-07 | global batch size:   256 | lm loss: 2.868090E+00 | loss scale: 1.0 | grad norm: 13.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:32:20][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       45/      50 | consumed samples:        11520 | elapsed time per iteration (ms): 22531.7/22497.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.8/674.8 | tokens per GPU (tokens/s/GPU): 11634.5/11652.5 | learning rate: 2.653493E-07 | global batch size:   256 | lm loss: 2.806202E+00 | loss scale: 1.0 | grad norm: 12.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:32:43][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       46/      50 | consumed samples:        11776 | elapsed time per iteration (ms): 22534.8/22497.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11632.8/11652.1 | learning rate: 1.703709E-07 | global batch size:   256 | lm loss: 2.790954E+00 | loss scale: 1.0 | grad norm: 12.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:33:05][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       47/      50 | consumed samples:        12032 | elapsed time per iteration (ms): 22533.1/22498.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11633.7/11651.6 | learning rate: 9.607360E-08 | global batch size:   256 | lm loss: 2.741159E+00 | loss scale: 1.0 | grad norm: 12.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:33:28][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       48/      50 | consumed samples:        12288 | elapsed time per iteration (ms): 22534.5/22499.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11633.0/11651.2 | learning rate: 4.277569E-08 | global batch size:   256 | lm loss: 2.793205E+00 | loss scale: 1.0 | grad norm: 11.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:33:50][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       49/      50 | consumed samples:        12544 | elapsed time per iteration (ms): 22524.3/22500.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.7 | tokens per GPU (tokens/s/GPU): 11638.3/11651.0 | learning rate: 1.070538E-08 | global batch size:   256 | lm loss: 2.646500E+00 | loss scale: 1.0 | grad norm: 12.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
[20251218 08:34:13][root/amd][pre_trainer][ip-10.21.9.25][rank-7/8][INFO ] [--------trainer.py:2578] :  iteration       50/      50 | consumed samples:        12800 | elapsed time per iteration (ms): 22532.0/22500.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.8/674.7 | tokens per GPU (tokens/s/GPU): 11634.3/11650.6 | learning rate: 0.000000E+00 | global batch size:   256 | lm loss: 2.718093E+00 | loss scale: 1.0 | grad norm: 11.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
