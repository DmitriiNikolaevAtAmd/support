W1218 08:35:10.250000 5603 torch/distributed/run.py:853] 
W1218 08:35:10.250000 5603 torch/distributed/run.py:853] *****************************************
W1218 08:35:10.250000 5603 torch/distributed/run.py:853] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 08:35:10.250000 5603 torch/distributed/run.py:853] *****************************************
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
[92mSuccessfully preprocessed all matching files.[0m
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
[92mSuccessfully preprocessed all matching files.[0m
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:485]: MegatronTrainer: monkey patch TopKRouter...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:356]: MegatronTrainer: monkey patch get_extra_te_kwargs...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:588]: MegatronTrainer: Patching FileSystemWriterAsync...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:600]: MegatronTrainer: Patch FileSystemWriterAsync successfully.[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:282]: MegatronTrainer: Patch get_fp8_context...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:694] : -run update_primus_config...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:810] : -rank:              0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:811] : -local_rank:        0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:812] : -world_size:        8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:829] : -save:              /workspace/Primus/output/amd/root/qwen2.5_7B-pretrain/checkpoints[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:836] : -auto_continue_train:0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:872] : -disable_tensorboard:True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:873] :   -tensorboard_dir: None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------trainer.py:889] : args.wandb_project is disabled, as args.disable_wandb=True.[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:890] : -disable_wandb:     True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:896] :   -wandb_project:   None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:897] :   -wandb_exp_name:  None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:898] :   -wandb_save_dir:  None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:899] :   -wandb_entity:    None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:902] : -disable_mlflow:    True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:916] :   -mlflow_run_name: None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:917] :   -mlflow_experiment_name:None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded model_provider from model_provider[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded gpt_builder from gpt_builders[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:702] : -run initialize_megatron...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1158] : -load:              None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1159] : -use_checkpoint_args:False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:416] : using world size: 8, data-parallel size: 8, context-parallel size: 1, hierarchical context-parallel sizes: None, tensor-model-parallel size: 1, pipeline-model-parallel size: 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:598] : Number of virtual stages per pipeline stage: None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:722] : accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:733] : using torch.bfloat16 for parameters ...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1191] : ------------------------ arguments ------------------------[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   account_for_embedding_in_pipeline_split ......... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   account_for_loss_in_pipeline_split .............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   accumulate_allreduce_grads_in_fp32 .............. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_beta1 ...................................... 0.9[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_beta2 ...................................... 0.95[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_eps ........................................ 1e-08[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_bias_linear ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_position_embedding .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_qkv_bias .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adlr_autoresume ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adlr_autoresume_interval ........................ 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   align_grad_reduce ............................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   align_param_gather .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   allow_padding_num_layers ........................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   app_tag_run_name ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   app_tag_run_version ............................. 0.0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_layernorm_1p .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_query_key_layer_scaling ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_residual_connection_post_layernorm ........ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_rope_fusion ............................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   async_save ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   async_tensor_model_parallel_allreduce ........... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_backend ............................... auto[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_dropout ............................... 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_softmax_in_fp32 ....................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attn_logit_softcapping .......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_continue_train ............................. 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_detect_ckpt_format ......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_offload_time ............................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   barrier_with_L1_time ............................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_binary_head ................................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_embedder_type .............................. megatron[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_load ....................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bf16 ............................................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_dropout_fusion ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_gelu_fusion ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_swiglu_fusion .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   biencoder_projection_dim ........................ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   biencoder_shared_query_context_model ............ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   block_data_path ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   calc_ft_timeouts ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   calculate_per_token_loss ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_large_grads ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_nan_in_loss_and_grad .................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_spiky_loss ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_weight_hash_across_dp_replicas_interval ... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_assume_constant_structure .................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_format ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_save ............................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_update_legacy_dist_opt_format ...... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_format ..................................... torch[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_load ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_save ........................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_save_deprecated ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_step ....................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   classes_fraction ................................ 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   clip_grad ....................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   clone_scatter_output_in_embedding ............... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   config_logger_dir ............................... [0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   consumed_train_samples .......................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   consumed_valid_samples .......................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   context_parallel_size ........................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cp_comm_type .................................... p2p[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cpu_offload ..................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   create_attention_mask_in_dataloader ............. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cross_entropy_fusion_impl ....................... native[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cross_entropy_loss_fusion ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cuda_graph_scope ................................ full[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cuda_graph_warmup_steps ......................... 3[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_args_path .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_cache_path ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_random_init ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_sharding_strategy ................. no_shard[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_size .............................. 8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_path ....................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_per_class_fraction ......................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_sharding ................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dataloader_type ................................. cyclic[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_average_in_collective ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_bucket_size ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_num_buckets ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_pad_buckets_for_high_nccl_busbw ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   debug_scheduler_table ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_first_pipeline_num_layers ............... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_last_pipeline_num_layers ................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_num_layers .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_pipeline_manual_split_list .............. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_seq_length .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoupled_lr .................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoupled_min_lr ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decrease_batch_size_if_needed ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   defer_embedding_wgrad_compute ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   delay_wgrad_compute ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   deprecated_use_mcore_models ..................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   deterministic_mode .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_bottleneck_size ............................ 256[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_freeze_last_layer .......................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_head_hidden_size ........................... 2048[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_local_crops_number ......................... 10[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_local_img_size ............................. 96[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_norm_last_layer ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_teacher_temp ............................... 0.07[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_warmup_teacher_temp ........................ 0.04[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_warmup_teacher_temp_epochs ................. 30[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_compile_dependencies .................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_last_saving ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_mamba_mem_eff_path ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_mlflow .................................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_primus_topk_router ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_profiler_activity_cpu ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_straggler_on_startup .................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_tensorboard ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_wandb ................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_format_deprecated ..................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_optim_fully_reshardable ............... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_save_pre_mcore_014 .................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_strictness ............................ assume_ok_unexpected[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distribute_saved_activations .................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distributed_backend ............................. nccl[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distributed_timeout_minutes ..................... 60[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dump_pp_data .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   embedding_path .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   empty_unused_memory_level ....................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_1f1b_v ................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_cuda_graph ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_exactly_numeric_match .................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_experimental ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_ft_package ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_gloo_process_groups ...................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_one_logger ............................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_optimizer_post_validation ................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_primus_turbo ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_turbo_attention_float8 ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_zb_runtime ............................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_zero_bubble .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_num_layers .............................. 28[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_pipeline_model_parallel_size ............ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_seq_length .............................. 2048[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_tensor_model_parallel_size .............. 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   end_weight_decay ................................ 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eod_mask_loss ................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   error_injection_rate ............................ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   error_injection_type ............................ transient_error[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eval_interval ................................... 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eval_iters ...................................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   evidence_data_path .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_duration_in_mins ........................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_interval ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_on_missing_checkpoint ...................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_signal_handler ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exp_avg_dtype ................................... torch.float32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exp_avg_sq_dtype ................................ torch.float32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   expert_model_parallel_size ...................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   expert_tensor_parallel_size ..................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   external_cuda_graph ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ffn_hidden_size ................................. 18944[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   file_sink_level ................................. DEBUG[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   final_logit_softcapping ......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   finetune ........................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   first_last_layers_bf16 .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   flash_decode .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp16 ............................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp16_lm_cross_entropy ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp32_residual_connection ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4 ............................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4_param ....................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4_recipe ...................................... nvfp4[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8 ............................................. hybrid[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_amax_compute_algo ........................... max[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_amax_history_len ............................ 1024[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_interval .................................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_margin ...................................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_param_gather ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_recipe ...................................... delayed[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_wgrad ....................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   framework ....................................... megatron[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   full_validation ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fused_padded_mla_attention ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   global_batch_size ............................... 256[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grad_reduce_in_bf16 ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   gradient_accumulation_fusion .................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   gradient_reduce_div_fusion ...................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   group_query_attention ........................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_clamp_eps_lower ............................ 0.01[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_clamp_eps_upper ............................ 0.01[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_default_temperature ........................ 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_default_top_p .............................. 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_entropy_term_weight ........................ 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_filter_groups_with_same_reward ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_group_size ................................. 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_iterations ................................. 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_kl_beta .................................... 0.001[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_prompts_per_step ........................... 32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   head_lr_mult .................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   heterogeneous_layers_config_encoded_json ........ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   heterogeneous_layers_config_path ................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hidden_dropout .................................. 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hidden_size ..................................... 3584[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hierarchical_context_parallel_sizes ............. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   high_priority_stream_groups ..................... [][0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_attention_ratio .......................... 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_mlp_ratio ................................ 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_override_pattern ......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hysteresis ...................................... 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ict_head_size ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ict_load ........................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   img_h ........................................... 224[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   img_w ........................................... 224[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   indexer_batch_size .............................. 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   indexer_log_interval ............................ 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_batch_times_seqlen_threshold .......... -1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_guaranteed_fraction  0.2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_overflow_factor  None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_size_gb ....... 40.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_max_requests_override  None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_max_tokens_override .. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_max_requests .......................... 8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_max_seq_length ........................ 2560[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_rng_tracker ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_method_std ................................. 0.008[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_method_xavier_uniform ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_model_with_meta_device ..................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   initial_loss_scale .............................. 4294967296[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inprocess_restart ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   interleave_group_size ........................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   is_hybrid_model ................................. False[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   iter_per_epoch .................................. 1250[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   iterations_to_skip .............................. [][0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   keep_fp8_transpose_cache_when_using_custom_fsdp . False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   kv_channels ..................................... 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   kv_lora_rank .................................... 32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_env_config ............................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_inference_server_conversation_template ... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_inference_server_type .................... inplace_megatron[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lazy_mpu_init ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   legacy_tokenizer ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   load ............................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   load_main_params_from_ckpt ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   local_rank ...................................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_avg_reset_interval .......................... 50[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_avg_skip_iterations ......................... 2[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_batch_size_to_tensorboard ................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_interval .................................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_learning_rate_to_tensorboard ................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_loss_scale_to_tensorboard ................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_memory_to_tensorboard ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_num_zeros_in_grad ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_params_norm ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_progress .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_straggler ................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_throughput .................................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_timers_to_tensorboard ....................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_validation_ppl_to_tensorboard ............... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_world_size_to_tensorboard ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   logging_level ................................... 10[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   loss_scale ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   loss_scale_window ............................... 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr .............................................. 1e-05[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_iters .................................. 320000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_samples ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_style .................................. cosine[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_fraction .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_init .................................. 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_iters ................................. 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_samples ............................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_iters .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_samples ............................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_style .............................. exponential[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   main_grads_dtype ................................ torch.float32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   main_params_dtype ............................... torch.float32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   make_vocab_size_divisible_by .................... 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_head_dim .................................. 64[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_num_groups ................................ 8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_num_heads ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_state_dim ................................. 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc ....................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc_eval .................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc_interval .............................. 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_factor ..................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_prob ....................................... 0.15[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_type ....................................... random[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   masked_softmax_fusion ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   max_position_embeddings ......................... 131072[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   max_tokens_to_oom ............................... 12000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   memory_snapshot_path ............................ snapshot.pickle[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   merge_file ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   micro_batch_size ................................ 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   microbatch_group_size_per_vp_stage .............. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   min_loss_scale .................................. 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   min_lr .......................................... 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mlflow_experiment_name .......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mlflow_run_name ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mmap_bin_files .................................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mock_data ....................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_aux_loss_coeff .............................. 0.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_enable_deepep ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_expert_capacity_factor ...................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_extended_tp ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_ffn_hidden_size ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_grouped_gemm ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_input_jitter_eps ............................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_layer_freq .................................. 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_layer_recompute ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_pad_expert_input_to_capacity ................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_per_layer_logging ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_permute_fusion .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_bias_update_rate ..................... 0.001[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_dtype ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_enable_expert_bias ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_force_load_balancing ................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_group_topk ........................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_load_balancing_type .................. aux_loss[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_num_groups ........................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_pre_softmax .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_score_function ....................... softmax[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_topk ................................. 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_topk_scaling_factor .................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_shared_expert_intermediate_size ............. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_shared_expert_overlap ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_token_dispatcher_type ....................... allgather[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_token_drop_policy ........................... probs[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_fused_router_with_aux_score ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_legacy_grouped_gemm ..................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_upcycling ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_z_loss_coeff ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mscale .......................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mscale_all_dim .................................. 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mtp_loss_scaling_factor ......................... 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mtp_num_layers .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   multi_latent_attention .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   multiple_validation_sets ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   name ............................................ pre_trainer[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   nccl_communicator_config_path ................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_fp8_weight_transpose_cache ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_load_optim ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_load_rng ..................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_persist_layer_norm ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_save_optim ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_save_rng ..................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_ckpt_type ........................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_global_ckpt_dir .................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_local_ckpt_algo .................. fully_parallel[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_local_ckpt_dir ................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_save_interval .................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   norm_epsilon .................................... 1e-06[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   normalization ................................... RMSNorm[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_attention_heads ............................. 28[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_channels .................................... 3[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_classes ..................................... 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_dataset_builder_threads ..................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_distributed_optimizer_instances ............. 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_experts ..................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers ...................................... 28[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_at_end_in_bf16 ....................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_at_start_in_bf16 ..................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_per_virtual_pipeline_stage ........... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_query_groups ................................ 4[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_seq_splits .................................. 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_virtual_stages_per_pipeline_rank ............ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_workers ..................................... 8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_chunk_num ............................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_overlap_sr .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_time .................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_async ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_project .............................. megatron-lm[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_run_name ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   onnx_safe ....................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   openai_gelu ..................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer ....................................... adam[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer_cpu_offload ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer_offload_fraction ...................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   output_bert_embeddings .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_cpu_optimizer_d2h_h2d ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_grad_reduce ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_moe_expert_parallel_comm ................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_p2p_comm ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_param_gather ............................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_param_gather_with_optimizer_step ........ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   override_opt_param_scheduler .................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   parallel_output ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   params_dtype .................................... torch.bfloat16[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_dim ....................................... 16[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_moe_overlap ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_zero_bubble ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   per_split_data_args_path ........................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   perform_initialization .......................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   perform_rl_step ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pin_cpu_grads ................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pin_cpu_params .................................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_comm_backend ............ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_layout .................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_size .................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_split_rank .............. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   position_embedding_type ......................... rope[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pp_warmup ....................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pre_communication_optimization .................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pretrained_checkpoint ........................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile ......................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_memory_iter ............................. -1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_ranks ................................... [0][0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_step_end ................................ 12[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_step_start .............................. 10[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   q_lora_rank ..................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_head_dim ..................................... 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_l2_norm ...................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_layernorm .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_pos_emb_head_dim ............................. 64[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   query_in_block_prob ............................. 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   quick_geglu ..................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rampup_batch_size ............................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rank ............................................ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_granularity ........................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_layer_ids ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_method ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_num_layers ............................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   record_memory_history ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication ..................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication_factor .............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication_jump ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rerun_mode ...................................... disabled[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   reset_attention_mask ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   reset_position_ids .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   result_rejected_tracker_filename ................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_report_topk_accuracies ................ [][0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_score_scaling ......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_seq_length ............................ 256[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_add_retriever ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_attention_gate ............................ 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_cyclic_train_iters ........................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_attention_dropout ................. 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_hidden_dropout .................... 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_layers ............................ 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_num_neighbors ............................. 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_num_retrieved_chunks ...................... 2[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_project_dir ............................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_verify_neighbor_count ..................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_calculate_intra_group_similarity ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_importance_sampling_truncation_coef .......... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_inference_logprobs_is_correction ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_offload_kv_cache_during_training ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_offload_optimizer_during_inference ........... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_partial_rollouts ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_prompts_per_eval ............................. 32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_remove_kv_cache_during_training .............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_reset_cuda_graphs ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rope_scaling_factor ............................. 8.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rope_type ....................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_base ..................................... 1000000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_interleaved .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_percent .................................. 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_scaling_factor ........................... 40[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_seq_len_interpolation_factor ............. 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   router_logit_softcapping ........................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   run_workload_inspector_server ................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   s3_cache_path ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sample_rate ..................................... 1.0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save ............................................ /workspace/Primus/output/amd/root/qwen2.5_7B-pretrain/checkpoints[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save_interval ................................... 20000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save_retain_interval ............................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   scatter_gather_tensors_in_pipeline .............. True[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   seed ............................................ 1234[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   seq_length ...................................... 2048[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sequence_parallel ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sgd_momentum .................................... 0.9[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sharp_enabled_group ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   short_seq_prob .................................. 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sink_level ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   skip_train ...................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   skipped_train_samples ........................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   spec ............................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   split ........................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   squared_relu .................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   standalone_embedding_stage ...................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   start_weight_decay .............................. 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   stderr_sink_level ............................... DEBUG[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   straggler_ctrlr_port ............................ 65535[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   straggler_minmax_count .......................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   suggested_communication_unit_size ............... 400000000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   swiglu .......................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   swin_backbone_type .............................. tiny[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   te_rng_tracker .................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensor_model_parallel_size ...................... 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_dir ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_log_interval ........................ 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_queue_size .......................... 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   test_data_path .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   test_mode ....................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_num_special_tokens ..................... 1000[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_pattern ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_special_tokens ......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   timing_log_level ................................ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   timing_log_option ............................... minmax[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   titles_data_path ................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tokenizer_model ................................. Qwen/Qwen2.5-7B[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tokenizer_type .................................. HuggingFaceTokenizer[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_record_shapes .................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_use_gzip ......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_with_stack ....................... True[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bootstrap_backend ....................... nccl[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bulk_dgrad .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bulk_wgrad .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_ag .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_cfg ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_rs .............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_rs_dgrad ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_split_ag ................................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_split_rs ................................ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_data_path ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_iters ..................................... 50[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_samples ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_sync_interval ............................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   trainable ....................................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   transformer_impl ................................ transformer_engine[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   transformer_pipeline_model_parallel_size ........ 1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   trust_remote_code ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_deepep_num_cu ............................. 32[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_deepep_use_comm_stream .................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_sync_free_moe_stage ....................... 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   untie_embeddings_and_output_weights ............. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_checkpoint_args ............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_checkpoint_opt_param_scheduler .............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_cpu_initialization .......................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_custom_fsdp ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_deprecated_20241209_moe_layer ............... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_dist_ckpt ................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_dist_ckpt_deprecated ........................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_distributed_optimizer ....................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_flash_attn .................................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_legacy_models ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_megatron_fsdp ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_mp_args_from_checkpoint_args ................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_one_sent_docs ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_persistent_ckpt_worker ...................... False[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_precision_aware_optimizer ................... False[0m

[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_pytorch_profiler ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_ring_exchange_p2p ........................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rocm_mem_info ............................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rocm_mem_info_iters ......................... [1, 2][0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rope_scaling ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rotary_position_embeddings .................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_sharp ....................................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_te_activation_func .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_tokenizer_model_from_checkpoint_args ........ True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_torch_fsdp2 ................................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_torch_optimizer_for_cpu_offload ............. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_tp_pp_dp_mapping ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_attention ............................. True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_deepep ................................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_fused_act_with_probs .................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_grouped_mlp ........................... True[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_parallel_linear ....................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_rms_norm .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   v_head_dim ...................................... 128[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   valid_data_path ................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   variable_seq_lengths ............................ False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   virtual_pipeline_model_parallel_size ............ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_backbone_type ............................ vit[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_pretraining .............................. False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_pretraining_type ......................... classify[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_extra_ids ................................. 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_file ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_size ...................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_entity .................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_exp_name .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_project ................................... None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_save_dir .................................. None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   weight_decay .................................... 0.1[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   weight_decay_incr_style ......................... constant[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wgrad_deferral_limit ............................ 0[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   world_size ...................................... 8[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   yaml_cfg ........................................ None[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_adaptive_memory_limit_percentile .... 85[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_max_pending_backward ................ auto[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_pipeline_timers_end_iter ............ 110[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_pipeline_timers_start_iter .......... 100[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_v_schedule .......................... False[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_v_schedule_mem_setup ................ half[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1199] : -------------------- end of arguments ---------------------[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1185] : -monkey patch megatron.training.global_vars._set_wandb_writer...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1190] : -set_global_variables...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1192] : -set_primus_global_variables...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1197] : -build_tokenizer...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------tokenizer.py:40] : -building HuggingFaceTokenizer tokenizer...[0m
[[32m20251218 08:35:25[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[--------tokenizer.py:24] : > building HuggingFaceTokenizer tokenizer ...[0m
[[32m20251218 08:35:25[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----global_vars.py:236] : WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------tokenizer.py:125] :  > padded vocab (size: 151665) with 15 dummy tokens (new size: 151680)[0m
RerunStateMachine initialized in mode disabled
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1250] : -lazy_mpu_init:     None[0m
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1227] : -initialize_distributed...[0m
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:329] : > initializing torch distributed ...[0m
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6[Gloo] Rank  is connected to 7 peer ranks. Expected number of connected peer ranks is : 74
 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:380] : > initialized tensor model parallel with size 1[0m
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:384] : > initialized pipeline model parallel with size 1[0m
[[32m20251218 08:35:26[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1231] : -seeds:             1234[0m
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:761] : time to initialize megatron (seconds): 12.249[0m
/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after megatron is initialized] datetime: 2025-12-18 08:35:36 [0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:994] : -setup_model_and_optimizer...[0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:243]: MegatronTrainer: patch TESpecProvider to PrimusTurboSpecProvider, `enable_primus_turbo=True` will use PrimusTurbo backend[0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1299] : use pt backend...[0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1307] : -run get_model[0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-1/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-5/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-2/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-4/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-3/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-6/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:35:36[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : building GPT model ...[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[--------training.py:903] :  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 7612734976[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1309] : [DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-27): 28 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): PrimusTurboAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear(in_features=3584, out_features=3584, bias=False, TP=1)
              (linear_qkv): TELayerNormColumnParallelLinear(in_features=3584, out_features=4608, bias=False, TP=1)
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): IdentityOp()
            (mlp): MLP(
              (linear_fc1): TELayerNormColumnParallelLinear(in_features=3584, out_features=37888, bias=False, TP=1)
              (linear_fc2): TERowParallelLinear(in_features=18944, out_features=3584, bias=False, TP=1)
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear(in_features=3584, out_features=151680, bias=False, TP=1)
    )
  )
)][0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1323] : -run get_megatron_optimizer[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after model, optimizer, and learning rate scheduler are built] datetime: 2025-12-18 08:35:37 [0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : > building train, validation, and test datasets ...[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :  > datasets target sizes (minimum size):[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     train:      12800[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     validation: 0[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     test:       0[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1111] : > building train, validation, and test datasets for GPT ...[0m
Unable to save MockGPTDataset indexes because path_to_cache is None
Unable to save MockGPTDataset indexes because path_to_cache is None
Unable to save MockGPTDataset indexes because path_to_cache is None
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1119] : > finished creating GPT datasets ...[0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after dataloaders are built] datetime: 2025-12-18 08:35:37 [0m
[[32m20251218 08:35:37[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1058] : done with setup ...[0m
[[32m20251218 08:35:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1453] : training ...[0m
[[32m20251218 08:35:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1597] : Setting rerun_state_machine.current_iteration to 0...[0m
[[32m20251218 08:35:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded FullyShardedDataParallel from megatron.core.distributed.fsdp.mcore_fsdp_adapter[0m
[[32m20251218 08:35:38[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[----------timers.py:430] : (min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (790.09, 790.17)
    train/valid/test-data-iterators-setup ..........: (8.44, 9.66)[0m
[[32m20251218 08:35:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [before the start of training step] datetime: 2025-12-18 08:35:38 [0m
[[32m20251218 08:35:40[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[fp8_utils.py:156]: WARNING: Primus-Turbo FP8 delayed not work since Primus-Turbo not support delayed scaling..[0m
[aiter] start build [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/build/module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_fmha_v3_fwd
[aiter] [32mfinish build [module_fmha_v3_fwd], cost 29.4s [0m
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [module_fmha_v3_fwd] under /workspace/aiter/aiter/jit/module_fmha_v3_fwd.so
[aiter] type hints mismatch, override to --> fmha_v3_fwd(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] start build [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/build/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic
[aiter] [32mfinish build [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic], cost 33.0s [0m
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
[aiter] import [mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic] under /workspace/aiter/aiter/jit/mha_bwd_bf16_nbias_ndbias_mask_ndropout_ndeterministic.so
[aiter] type hints mismatch, override to --> mha_bwd(dout: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, softmax_lse: torch.Tensor, dropout_p: float, softmax_scale: float, is_causal: bool, window_size_left: int, window_size_right: int, deterministic: bool, dq: Optional[torch.Tensor] = None, dk: Optional[torch.Tensor] = None, dv: Optional[torch.Tensor] = None, dbias: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, rng_state: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> List[torch.Tensor]
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:137] : Number of parameters in transformer block in billions:  6.53[0m
[[32m20251218 08:37:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        1/      50 | consumed samples:          256 | elapsed time per iteration (ms): 87713.3/87713.3 | hip mem usage/free/total/usage_ratio: 89.46GB/102.53GB/191.98GB/46.60% | rocm mem usage/free/total/usage_ratio: 89.94GB/102.04GB/191.98GB/46.85% | throughput per GPU (TFLOP/s/GPU): 33.5/33.5 | tokens per GPU (tokens/s/GPU): 747.2/747.2 | learning rate: 5.000000E-06 | global batch size:   256 | lm loss: 1.204098E+01 | loss scale: 1.0 | grad norm: 3.307 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:146] : Number of parameters in embedding layers in billions: 1.09[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:150] : Total number of parameters in billions: 7.61[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:163] : Number of parameters in most loaded shard in billions: 7.6129[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : compute_activation_memory_without_sp[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:270] : Activation memory footprint per transformer layer (precise, without SP): 476.0 MB[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:362] : Theoretical memory footprints: weight and optimizer=54451.98 MB, activation=15282.78 MB, total=69734.77 MB
[0m
[[32m20251218 08:37:05[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:275] : [Rank 0] (after 1 iterations) memory (MB) | allocated: 67118.81884765625 | max allocated: 78397.279296875 | reserved: 81684.0 | max reserved: 81684.0[0m
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

[[32m20251218 08:37:11[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        2/      50 | consumed samples:          512 | elapsed time per iteration (ms): 5202.6/46457.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | rocm mem usage/free/total/usage_ratio: 95.44GB/96.54GB/191.98GB/49.71% | throughput per GPU (TFLOP/s/GPU): 565.3/299.4 | tokens per GPU (tokens/s/GPU): 12596.8/6672.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.204219E+01 | loss scale: 1.0 | grad norm: 3.653 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:16[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        3/      50 | consumed samples:          768 | elapsed time per iteration (ms): 5014.6/5014.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 586.5/586.5 | tokens per GPU (tokens/s/GPU): 13068.9/13068.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.148777E+01 | loss scale: 1.0 | grad norm: 3.701 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:21[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        4/      50 | consumed samples:         1024 | elapsed time per iteration (ms): 5022.3/5018.5 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 585.6/586.1 | tokens per GPU (tokens/s/GPU): 13048.9/13058.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.041016E+01 | loss scale: 1.0 | grad norm: 4.795 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:26[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        5/      50 | consumed samples:         1280 | elapsed time per iteration (ms): 5032.4/5023.1 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 584.5/585.5 | tokens per GPU (tokens/s/GPU): 13022.7/13046.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 9.059455E+00 | loss scale: 1.0 | grad norm: 6.169 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:31[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        6/      50 | consumed samples:         1536 | elapsed time per iteration (ms): 5043.2/5028.2 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 583.2/585.0 | tokens per GPU (tokens/s/GPU): 12994.8/13033.8 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 7.250917E+00 | loss scale: 1.0 | grad norm: 7.152 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:36[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        7/      50 | consumed samples:         1792 | elapsed time per iteration (ms): 5051.7/5032.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 582.2/584.4 | tokens per GPU (tokens/s/GPU): 12973.2/13021.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 5.908051E+00 | loss scale: 1.0 | grad norm: 7.619 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:41[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        8/      50 | consumed samples:         2048 | elapsed time per iteration (ms): 5063.1/5037.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 580.9/583.8 | tokens per GPU (tokens/s/GPU): 12943.9/13008.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 4.241145E+00 | loss scale: 1.0 | grad norm: 7.450 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:46[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        9/      50 | consumed samples:         2304 | elapsed time per iteration (ms): 5079.2/5043.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 579.1/583.1 | tokens per GPU (tokens/s/GPU): 12902.8/12993.6 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.860539E+00 | loss scale: 1.0 | grad norm: 6.179 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:51[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       10/      50 | consumed samples:         2560 | elapsed time per iteration (ms): 5082.7/5048.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 578.7/582.6 | tokens per GPU (tokens/s/GPU): 12894.0/12981.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.186695E+00 | loss scale: 1.0 | grad norm: 4.505 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:37:56[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       11/      50 | consumed samples:         2816 | elapsed time per iteration (ms): 5093.2/5053.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 577.5/582.0 | tokens per GPU (tokens/s/GPU): 12867.3/12968.5 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.600620E+00 | loss scale: 1.0 | grad norm: 3.558 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:01[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       12/      50 | consumed samples:         3072 | elapsed time per iteration (ms): 5095.8/5057.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 577.2/581.5 | tokens per GPU (tokens/s/GPU): 12860.9/12957.8 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.124073E+00 | loss scale: 1.0 | grad norm: 2.265 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:06[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       13/      50 | consumed samples:         3328 | elapsed time per iteration (ms): 5108.0/5062.4 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.8/581.0 | tokens per GPU (tokens/s/GPU): 12830.1/12946.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 9.071150E-01 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:11[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       14/      50 | consumed samples:         3584 | elapsed time per iteration (ms): 5114.2/5066.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.1/580.5 | tokens per GPU (tokens/s/GPU): 12814.5/12935.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 6.495297E-01 | loss scale: 1.0 | grad norm: 1.432 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:16[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       15/      50 | consumed samples:         3840 | elapsed time per iteration (ms): 5117.2/5070.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 574.8/580.1 | tokens per GPU (tokens/s/GPU): 12807.1/12925.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 4.634443E-01 | loss scale: 1.0 | grad norm: 1.101 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:22[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       16/      50 | consumed samples:         4096 | elapsed time per iteration (ms): 5120.3/5074.1 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 574.4/579.7 | tokens per GPU (tokens/s/GPU): 12799.4/12916.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.953719E-01 | loss scale: 1.0 | grad norm: 0.785 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:27[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       17/      50 | consumed samples:         4352 | elapsed time per iteration (ms): 5127.1/5077.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 573.7/579.3 | tokens per GPU (tokens/s/GPU): 12782.2/12907.4 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.515750E-01 | loss scale: 1.0 | grad norm: 0.753 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:32[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       18/      50 | consumed samples:         4608 | elapsed time per iteration (ms): 5128.7/5080.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 573.5/578.9 | tokens per GPU (tokens/s/GPU): 12778.4/12899.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.753721E-01 | loss scale: 1.0 | grad norm: 0.590 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:37[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       19/      50 | consumed samples:         4864 | elapsed time per iteration (ms): 5127.7/5083.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 573.6/578.6 | tokens per GPU (tokens/s/GPU): 12780.8/12892.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.257071E-01 | loss scale: 1.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:42[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       20/      50 | consumed samples:         5120 | elapsed time per iteration (ms): 5118.8/5085.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 574.6/578.4 | tokens per GPU (tokens/s/GPU): 12803.1/12887.4 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.706159E-01 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:48[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       21/      50 | consumed samples:         5376 | elapsed time per iteration (ms): 5426.8/5103.5 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 542.0/576.5 | tokens per GPU (tokens/s/GPU): 12076.3/12844.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.904083E-01 | loss scale: 1.0 | grad norm: 0.397 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:53[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       22/      50 | consumed samples:         5632 | elapsed time per iteration (ms): 5400.2/5118.4 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 544.7/574.9 | tokens per GPU (tokens/s/GPU): 12135.8/12809.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.437280E-01 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:38:59[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       23/      50 | consumed samples:         5888 | elapsed time per iteration (ms): 6296.9/5174.5 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 467.1/569.7 | tokens per GPU (tokens/s/GPU): 10407.6/12694.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.198699E-01 | loss scale: 1.0 | grad norm: 0.248 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       24/      50 | consumed samples:         6144 | elapsed time per iteration (ms): 5402.2/5184.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 544.4/568.6 | tokens per GPU (tokens/s/GPU): 12131.3/12669.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.052888E-01 | loss scale: 1.0 | grad norm: 0.228 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:10[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       25/      50 | consumed samples:         6400 | elapsed time per iteration (ms): 5110.1/5181.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.6/568.9 | tokens per GPU (tokens/s/GPU): 12824.9/12676.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.058560E-01 | loss scale: 1.0 | grad norm: 0.247 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:15[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       26/      50 | consumed samples:         6656 | elapsed time per iteration (ms): 5114.1/5178.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.1/569.2 | tokens per GPU (tokens/s/GPU): 12814.8/12681.8 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 8.840220E-02 | loss scale: 1.0 | grad norm: 0.209 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       27/      50 | consumed samples:         6912 | elapsed time per iteration (ms): 5114.9/5176.2 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.0/569.4 | tokens per GPU (tokens/s/GPU): 12812.7/12687.1 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 7.957634E-02 | loss scale: 1.0 | grad norm: 0.182 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:25[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       28/      50 | consumed samples:         7168 | elapsed time per iteration (ms): 5106.6/5173.5 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.0/569.6 | tokens per GPU (tokens/s/GPU): 12833.6/12692.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 7.899091E-02 | loss scale: 1.0 | grad norm: 0.193 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:30[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       29/      50 | consumed samples:         7424 | elapsed time per iteration (ms): 5106.9/5171.1 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.9/569.9 | tokens per GPU (tokens/s/GPU): 12832.9/12697.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 6.544290E-02 | loss scale: 1.0 | grad norm: 0.159 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       30/      50 | consumed samples:         7680 | elapsed time per iteration (ms): 5104.0/5168.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.3/570.1 | tokens per GPU (tokens/s/GPU): 12840.0/12703.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 6.245424E-02 | loss scale: 1.0 | grad norm: 0.163 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:40[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       31/      50 | consumed samples:         7936 | elapsed time per iteration (ms): 5111.1/5166.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.5/570.3 | tokens per GPU (tokens/s/GPU): 12822.3/12707.1 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 6.245676E-02 | loss scale: 1.0 | grad norm: 0.154 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:45[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       32/      50 | consumed samples:         8192 | elapsed time per iteration (ms): 5106.7/5164.7 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.0/570.5 | tokens per GPU (tokens/s/GPU): 12833.2/12711.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 5.344947E-02 | loss scale: 1.0 | grad norm: 0.141 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:51[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       33/      50 | consumed samples:         8448 | elapsed time per iteration (ms): 5105.9/5162.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.0/570.7 | tokens per GPU (tokens/s/GPU): 12835.3/12715.3 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 5.024122E-02 | loss scale: 1.0 | grad norm: 0.136 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:39:56[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       34/      50 | consumed samples:         8704 | elapsed time per iteration (ms): 5106.0/5161.0 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.0/570.8 | tokens per GPU (tokens/s/GPU): 12835.1/12719.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 4.657329E-02 | loss scale: 1.0 | grad norm: 0.132 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:01[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       35/      50 | consumed samples:         8960 | elapsed time per iteration (ms): 5110.0/5159.5 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.6/571.0 | tokens per GPU (tokens/s/GPU): 12825.1/12722.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 4.374465E-02 | loss scale: 1.0 | grad norm: 0.130 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:06[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       36/      50 | consumed samples:         9216 | elapsed time per iteration (ms): 5110.9/5158.0 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.5/571.1 | tokens per GPU (tokens/s/GPU): 12822.8/12725.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.898568E-02 | loss scale: 1.0 | grad norm: 0.117 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:11[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       37/      50 | consumed samples:         9472 | elapsed time per iteration (ms): 5107.9/5156.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.8/571.2 | tokens per GPU (tokens/s/GPU): 12830.3/12728.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.813941E-02 | loss scale: 1.0 | grad norm: 0.115 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:16[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       38/      50 | consumed samples:         9728 | elapsed time per iteration (ms): 5107.9/5155.3 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.8/571.4 | tokens per GPU (tokens/s/GPU): 12830.2/12731.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.752861E-02 | loss scale: 1.0 | grad norm: 0.120 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:21[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       39/      50 | consumed samples:         9984 | elapsed time per iteration (ms): 5108.3/5154.0 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.8/571.5 | tokens per GPU (tokens/s/GPU): 12829.4/12733.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.345628E-02 | loss scale: 1.0 | grad norm: 0.107 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:26[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       40/      50 | consumed samples:        10240 | elapsed time per iteration (ms): 5102.3/5152.6 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.5/571.6 | tokens per GPU (tokens/s/GPU): 12844.4/12736.6 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.246421E-02 | loss scale: 1.0 | grad norm: 0.102 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:31[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       41/      50 | consumed samples:        10496 | elapsed time per iteration (ms): 5101.6/5151.3 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.5/571.7 | tokens per GPU (tokens/s/GPU): 12846.1/12739.4 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 3.038107E-02 | loss scale: 1.0 | grad norm: 0.102 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:37[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       42/      50 | consumed samples:        10752 | elapsed time per iteration (ms): 5103.5/5150.1 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.3/571.9 | tokens per GPU (tokens/s/GPU): 12841.3/12742.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.976460E-02 | loss scale: 1.0 | grad norm: 0.101 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:42[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       43/      50 | consumed samples:        11008 | elapsed time per iteration (ms): 5101.9/5149.0 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.5/572.0 | tokens per GPU (tokens/s/GPU): 12845.3/12744.5 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.775801E-02 | loss scale: 1.0 | grad norm: 0.096 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:47[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       44/      50 | consumed samples:        11264 | elapsed time per iteration (ms): 5101.4/5147.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.6/572.1 | tokens per GPU (tokens/s/GPU): 12846.7/12746.9 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.531127E-02 | loss scale: 1.0 | grad norm: 0.092 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:52[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       45/      50 | consumed samples:        11520 | elapsed time per iteration (ms): 5102.2/5146.8 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.5/572.2 | tokens per GPU (tokens/s/GPU): 12844.7/12749.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.348977E-02 | loss scale: 1.0 | grad norm: 0.090 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:40:57[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       46/      50 | consumed samples:        11776 | elapsed time per iteration (ms): 5107.8/5145.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 575.8/572.3 | tokens per GPU (tokens/s/GPU): 12830.6/12751.0 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.258862E-02 | loss scale: 1.0 | grad norm: 0.087 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:41:02[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       47/      50 | consumed samples:        12032 | elapsed time per iteration (ms): 5100.6/5144.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.6/572.4 | tokens per GPU (tokens/s/GPU): 12848.8/12753.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.296929E-02 | loss scale: 1.0 | grad norm: 0.086 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:41:07[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       48/      50 | consumed samples:        12288 | elapsed time per iteration (ms): 5098.2/5143.9 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 576.9/572.5 | tokens per GPU (tokens/s/GPU): 12854.7/12755.4 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.079840E-02 | loss scale: 1.0 | grad norm: 0.083 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:41:13[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       49/      50 | consumed samples:        12544 | elapsed time per iteration (ms): 5390.4/5149.1 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 545.6/571.9 | tokens per GPU (tokens/s/GPU): 12157.9/12742.7 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 2.069005E-02 | loss scale: 1.0 | grad norm: 0.084 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:41:18[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       50/      50 | consumed samples:        12800 | elapsed time per iteration (ms): 5240.1/5151.0 | hip mem usage/free/total/usage_ratio: 94.95GB/97.03GB/191.98GB/49.46% | throughput per GPU (TFLOP/s/GPU): 561.3/571.7 | tokens per GPU (tokens/s/GPU): 12506.7/12737.8 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.820078E-02 | loss scale: 1.0 | grad norm: 0.077 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:41:18[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after training is done] datetime: 2025-12-18 08:41:18 [0m
