W1218 08:14:08.590000 616 torch/distributed/run.py:853] 
W1218 08:14:08.590000 616 torch/distributed/run.py:853] *****************************************
W1218 08:14:08.590000 616 torch/distributed/run.py:853] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 08:14:08.590000 616 torch/distributed/run.py:853] *****************************************
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus CLI] HF_HOME already set: /workspace/Primus/data/huggingface
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
[Primus] sys.path.insert: /workspace/Primus/third_party/Megatron-LM
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
[92mSuccessfully preprocessed all matching files.[0m
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
Supported flash-attn versions are >= 2.1.1, <= 2.8.0.post2. Found flash-attn 2.8.3.
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/opt/venv/lib/python3.10/site-packages/torch/library.py:356: UserWarning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: flash_attn::_flash_attn_backward(Tensor dout, Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor(a6!)? dq, Tensor(a7!)? dk, Tensor(a8!)? dv, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left, SymInt window_size_right, float softcap, Tensor? alibi_slopes, bool deterministic, Tensor? rng_state=None) -> Tensor
    registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922
  dispatch key: ADInplaceOrView
  previous kernel: no debug info
       new kernel: registered at /opt/venv/lib/python3.10/site-packages/torch/_library/custom_ops.py:922 (Triggered internally at /pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:208.)
  self.m.impl(
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/inference/unified_memory.py:83: UserWarning: Failed to create unified memory mempool.
  warnings.warn("Failed to create unified memory mempool.")
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
/workspace/Primus/third_party/Megatron-LM/megatron/core/energy_monitor.py:9: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  from pynvml import (
[aiter] start build [module_aiter_enum] under /workspace/aiter/aiter/jit/build/module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] waiting for baton release at /workspace/aiter/aiter/jit/build/lock_module_aiter_enum
[aiter] [32mfinish build [module_aiter_enum], cost 7.1s [0m
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /workspace/aiter/aiter/jit/module_aiter_enum.so
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:14:38[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----global_vars.py:236] : WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[PrimusPatch] Applied Megatron build_model monkey-patch to disable second DDP.
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:485]: MegatronTrainer: monkey patch TopKRouter...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:356]: MegatronTrainer: monkey patch get_extra_te_kwargs...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:588]: MegatronTrainer: Patching FileSystemWriterAsync...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:600]: MegatronTrainer: Patch FileSystemWriterAsync successfully.[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:282]: MegatronTrainer: Patch get_fp8_context...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:694] : -run update_primus_config...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:810] : -rank:              0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:811] : -local_rank:        0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:812] : -world_size:        8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:829] : -save:              /workspace/Primus/output/amd/root/llama3.1_8B-pretrain/checkpoints[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:836] : -auto_continue_train:False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:872] : -disable_tensorboard:True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:873] :   -tensorboard_dir: None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------trainer.py:889] : args.wandb_project is disabled, as args.disable_wandb=True.[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:890] : -disable_wandb:     True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:896] :   -wandb_project:   None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:897] :   -wandb_exp_name:  None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:898] :   -wandb_save_dir:  None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:899] :   -wandb_entity:    None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:902] : -disable_mlflow:    True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:916] :   -mlflow_run_name: None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:917] :   -mlflow_experiment_name:None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded model_provider from model_provider[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded gpt_builder from gpt_builders[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:702] : -run initialize_megatron...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1158] : -load:              None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1159] : -use_checkpoint_args:False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:416] : using world size: 8, data-parallel size: 8, context-parallel size: 1, hierarchical context-parallel sizes: None, tensor-model-parallel size: 1, pipeline-model-parallel size: 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:598] : Number of virtual stages per pipeline stage: None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:722] : accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:733] : using torch.bfloat16 for parameters ...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1191] : ------------------------ arguments ------------------------[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   account_for_embedding_in_pipeline_split ......... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   account_for_loss_in_pipeline_split .............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   accumulate_allreduce_grads_in_fp32 .............. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_beta1 ...................................... 0.9[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_beta2 ...................................... 0.95[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adam_eps ........................................ 1e-08[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_bias_linear ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_position_embedding .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   add_qkv_bias .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adlr_autoresume ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   adlr_autoresume_interval ........................ 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   align_grad_reduce ............................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   align_param_gather .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   allow_padding_num_layers ........................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   app_tag_run_name ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   app_tag_run_version ............................. 0.0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_layernorm_1p .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_query_key_layer_scaling ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_residual_connection_post_layernorm ........ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   apply_rope_fusion ............................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   async_save ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   async_tensor_model_parallel_allreduce ........... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_backend ............................... auto[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_dropout ............................... 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attention_softmax_in_fp32 ....................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   attn_logit_softcapping .......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_continue_train ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_detect_ckpt_format ......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   auto_offload_time ............................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   barrier_with_L1_time ............................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_binary_head ................................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_embedder_type .............................. megatron[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bert_load ....................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bf16 ............................................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_dropout_fusion ............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_gelu_fusion ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   bias_swiglu_fusion .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   biencoder_projection_dim ........................ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   biencoder_shared_query_context_model ............ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   block_data_path ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   calc_ft_timeouts ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   calculate_per_token_loss ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_large_grads ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_nan_in_loss_and_grad .................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_for_spiky_loss ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   check_weight_hash_across_dp_replicas_interval ... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_assume_constant_structure .................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_format ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_save ............................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_convert_update_legacy_dist_opt_format ...... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_format ..................................... torch[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_load ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_save ........................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_fully_parallel_save_deprecated ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ckpt_step ....................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   classes_fraction ................................ 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   clip_grad ....................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   clone_scatter_output_in_embedding ............... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   config_logger_dir ............................... [0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   consumed_train_samples .......................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   consumed_valid_samples .......................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   context_parallel_size ........................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cp_comm_type .................................... p2p[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cpu_offload ..................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   create_attention_mask_in_dataloader ............. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cross_entropy_fusion_impl ....................... native[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cross_entropy_loss_fusion ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cuda_graph_scope ................................ full[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   cuda_graph_warmup_steps ......................... 3[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_args_path .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_cache_path ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_random_init ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_sharding_strategy ................. no_shard[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_parallel_size .............................. 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_path ....................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_per_class_fraction ......................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   data_sharding ................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dataloader_type ................................. cyclic[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_average_in_collective ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_bucket_size ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_num_buckets ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ddp_pad_buckets_for_high_nccl_busbw ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   debug_scheduler_table ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_first_pipeline_num_layers ............... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_last_pipeline_num_layers ................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_num_layers .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_pipeline_manual_split_list .............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoder_seq_length .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoupled_lr .................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decoupled_min_lr ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   decrease_batch_size_if_needed ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   defer_embedding_wgrad_compute ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   delay_wgrad_compute ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   deprecated_use_mcore_models ..................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   deterministic_mode .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_bottleneck_size ............................ 256[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_freeze_last_layer .......................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_head_hidden_size ........................... 2048[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_local_crops_number ......................... 10[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_local_img_size ............................. 96[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_norm_last_layer ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_teacher_temp ............................... 0.07[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_warmup_teacher_temp ........................ 0.04[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dino_warmup_teacher_temp_epochs ................. 30[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_compile_dependencies .................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_last_saving ............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_mamba_mem_eff_path ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_mlflow .................................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_primus_topk_router ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_profiler_activity_cpu ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_straggler_on_startup .................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_tensorboard ............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   disable_wandb ................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_format_deprecated ..................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_optim_fully_reshardable ............... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_save_pre_mcore_014 .................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dist_ckpt_strictness ............................ assume_ok_unexpected[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distribute_saved_activations .................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distributed_backend ............................. nccl[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   distributed_timeout_minutes ..................... 60[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   dump_pp_data .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   embedding_path .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   empty_unused_memory_level ....................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_1f1b_v ................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_cuda_graph ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_exactly_numeric_match .................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_experimental ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_ft_package ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_gloo_process_groups ...................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_one_logger ............................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_optimizer_post_validation ................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_primus_turbo ............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_turbo_attention_float8 ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_zb_runtime ............................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   enable_zero_bubble .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_num_layers .............................. 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_pipeline_model_parallel_size ............ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_seq_length .............................. 8192[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   encoder_tensor_model_parallel_size .............. 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   end_weight_decay ................................ 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eod_mask_loss ................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   error_injection_rate ............................ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   error_injection_type ............................ transient_error[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eval_interval ................................... 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   eval_iters ...................................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   evidence_data_path .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_duration_in_mins ........................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_interval ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_on_missing_checkpoint ...................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exit_signal_handler ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exp_avg_dtype ................................... torch.float32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   exp_avg_sq_dtype ................................ torch.float32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   expert_model_parallel_size ...................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   expert_tensor_parallel_size ..................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   external_cuda_graph ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ffn_hidden_size ................................. 14336[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   file_sink_level ................................. DEBUG[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   final_logit_softcapping ......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   finetune ........................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   first_last_layers_bf16 .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   flash_decode .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp16 ............................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp16_lm_cross_entropy ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp32_residual_connection ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4 ............................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4_param ....................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp4_recipe ...................................... nvfp4[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8 ............................................. hybrid[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_amax_compute_algo ........................... max[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_amax_history_len ............................ 1024[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_interval .................................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_margin ...................................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_param_gather ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_recipe ...................................... delayed[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fp8_wgrad ....................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   framework ....................................... megatron[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   full_validation ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   fused_padded_mla_attention ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   global_batch_size ............................... 256[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grad_reduce_in_bf16 ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   gradient_accumulation_fusion .................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   gradient_reduce_div_fusion ...................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   group_query_attention ........................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_clamp_eps_lower ............................ 0.01[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_clamp_eps_upper ............................ 0.01[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_default_temperature ........................ 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_default_top_p .............................. 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_entropy_term_weight ........................ 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_filter_groups_with_same_reward ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_group_size ................................. 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_iterations ................................. 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_kl_beta .................................... 0.001[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   grpo_prompts_per_step ........................... 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   head_lr_mult .................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   heterogeneous_layers_config_encoded_json ........ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   heterogeneous_layers_config_path ................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hidden_dropout .................................. 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hidden_size ..................................... 4096[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hierarchical_context_parallel_sizes ............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   high_priority_stream_groups ..................... [][0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_attention_ratio .......................... 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_mlp_ratio ................................ 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hybrid_override_pattern ......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   hysteresis ...................................... 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ict_head_size ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   ict_load ........................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   img_h ........................................... 224[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   img_w ........................................... 224[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   indexer_batch_size .............................. 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   indexer_log_interval ............................ 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_batch_times_seqlen_threshold .......... -1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_guaranteed_fraction  0.2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_overflow_factor  None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_buffer_size_gb ....... 40.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_max_requests_override  None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_dynamic_batching_max_tokens_override .. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_max_requests .......................... 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_max_seq_length ........................ 2560[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inference_rng_tracker ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_method_std ................................. 0.008[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_method_xavier_uniform ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   init_model_with_meta_device ..................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   initial_loss_scale .............................. 4294967296[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   inprocess_restart ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   interleave_group_size ........................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   is_hybrid_model ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   iter_per_epoch .................................. 1250[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   iterations_to_skip .............................. [][0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   keep_fp8_transpose_cache_when_using_custom_fsdp . False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   kv_channels ..................................... 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   kv_lora_rank .................................... 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_env_config ............................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_inference_server_conversation_template ... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   langrl_inference_server_type .................... inplace_megatron[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lazy_mpu_init ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   legacy_tokenizer ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   load ............................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   load_main_params_from_ckpt ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   local_rank ...................................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_avg_reset_interval .......................... 50[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_avg_skip_iterations ......................... 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_batch_size_to_tensorboard ................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_interval .................................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_learning_rate_to_tensorboard ................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_loss_scale_to_tensorboard ................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_memory_to_tensorboard ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_num_zeros_in_grad ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_params_norm ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_progress .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_straggler ................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_throughput .................................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_timers_to_tensorboard ....................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_validation_ppl_to_tensorboard ............... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   log_world_size_to_tensorboard ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   logging_level ................................... 10[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   loss_scale ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   loss_scale_window ............................... 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr .............................................. 1e-05[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_iters .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_samples ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_decay_style .................................. cosine[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_fraction .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_init .................................. 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_iters ................................. 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_warmup_samples ............................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_iters .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_samples ............................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   lr_wsd_decay_style .............................. exponential[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   main_grads_dtype ................................ torch.float32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   main_params_dtype ............................... torch.float32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   make_vocab_size_divisible_by .................... 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_head_dim .................................. 64[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_num_groups ................................ 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_num_heads ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mamba_state_dim ................................. 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc ....................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc_eval .................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   manual_gc_interval .............................. 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_factor ..................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_prob ....................................... 0.15[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mask_type ....................................... random[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   masked_softmax_fusion ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   max_position_embeddings ......................... 8192[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   max_tokens_to_oom ............................... 12000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   memory_snapshot_path ............................ snapshot.pickle[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   merge_file ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   micro_batch_size ................................ 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   microbatch_group_size_per_vp_stage .............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   min_loss_scale .................................. 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   min_lr .......................................... 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mlflow_experiment_name .......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mlflow_run_name ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mmap_bin_files .................................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mock_data ....................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_aux_loss_coeff .............................. 0.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_enable_deepep ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_expert_capacity_factor ...................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_extended_tp ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_ffn_hidden_size ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_grouped_gemm ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_input_jitter_eps ............................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_layer_freq .................................. 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_layer_recompute ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_pad_expert_input_to_capacity ................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_per_layer_logging ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_permute_fusion .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_bias_update_rate ..................... 0.001[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_dtype ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_enable_expert_bias ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_force_load_balancing ................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_group_topk ........................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_load_balancing_type .................. aux_loss[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_num_groups ........................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_pre_softmax .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_score_function ....................... softmax[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_topk ................................. 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_router_topk_scaling_factor .................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_shared_expert_intermediate_size ............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_shared_expert_overlap ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_token_dispatcher_type ....................... allgather[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_token_drop_policy ........................... probs[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_fused_router_with_aux_score ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_legacy_grouped_gemm ..................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_use_upcycling ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   moe_z_loss_coeff ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mscale .......................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mscale_all_dim .................................. 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mtp_loss_scaling_factor ......................... 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   mtp_num_layers .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   multi_latent_attention .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   multiple_validation_sets ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   name ............................................ pre_trainer[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   nccl_communicator_config_path ................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_fp8_weight_transpose_cache ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_load_optim ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_load_rng ..................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_persist_layer_norm ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_save_optim ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   no_save_rng ..................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_ckpt_type ........................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_global_ckpt_dir .................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_local_ckpt_algo .................. fully_parallel[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_local_ckpt_dir ................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   non_persistent_save_interval .................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   norm_epsilon .................................... 1e-06[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   normalization ................................... RMSNorm[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_attention_heads ............................. 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_channels .................................... 3[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_classes ..................................... 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_dataset_builder_threads ..................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_distributed_optimizer_instances ............. 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_experts ..................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers ...................................... 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_at_end_in_bf16 ....................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_at_start_in_bf16 ..................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_layers_per_virtual_pipeline_stage ........... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_query_groups ................................ 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_seq_splits .................................. 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_virtual_stages_per_pipeline_rank ............ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   num_workers ..................................... 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_chunk_num ............................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_overlap_sr .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   offload_time .................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_async ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_project .............................. megatron-lm[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   one_logger_run_name ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   onnx_safe ....................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   openai_gelu ..................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer ....................................... adam[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer_cpu_offload ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   optimizer_offload_fraction ...................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   output_bert_embeddings .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_cpu_optimizer_d2h_h2d ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_grad_reduce ............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_moe_expert_parallel_comm ................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_p2p_comm ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_param_gather ............................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   overlap_param_gather_with_optimizer_step ........ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   override_opt_param_scheduler .................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   parallel_output ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   params_dtype .................................... torch.bfloat16[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_dim ....................................... 16[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_moe_overlap ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   patch_zero_bubble ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   per_split_data_args_path ........................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   perform_initialization .......................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   perform_rl_step ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pin_cpu_grads ................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pin_cpu_params .................................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_comm_backend ............ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_layout .................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_size .................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pipeline_model_parallel_split_rank .............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   position_embedding_type ......................... rope[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pp_warmup ....................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pre_communication_optimization .................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   pretrained_checkpoint ........................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile ......................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_memory_iter ............................. -1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_ranks ................................... [0][0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_step_end ................................ 12[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   profile_step_start .............................. 10[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   q_lora_rank ..................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_head_dim ..................................... 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_l2_norm ...................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_layernorm .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   qk_pos_emb_head_dim ............................. 64[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   query_in_block_prob ............................. 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   quick_geglu ..................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rampup_batch_size ............................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rank ............................................ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_granularity ........................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_layer_ids ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_method ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   recompute_num_layers ............................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   record_memory_history ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication ..................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication_factor .............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   replication_jump ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rerun_mode ...................................... disabled[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   reset_attention_mask ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   reset_position_ids .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   result_rejected_tracker_filename ................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_report_topk_accuracies ................ [][0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_score_scaling ......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retriever_seq_length ............................ 256[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_add_retriever ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_attention_gate ............................ 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_cyclic_train_iters ........................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_attention_dropout ................. 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_hidden_dropout .................... 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_encoder_layers ............................ 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_num_neighbors ............................. 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_num_retrieved_chunks ...................... 2[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_project_dir ............................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   retro_verify_neighbor_count ..................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_calculate_intra_group_similarity ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_importance_sampling_truncation_coef .......... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_inference_logprobs_is_correction ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_offload_kv_cache_during_training ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_offload_optimizer_during_inference ........... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_partial_rollouts ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_prompts_per_eval ............................. 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_remove_kv_cache_during_training .............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rl_reset_cuda_graphs ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rope_scaling_factor ............................. 8.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rope_type ....................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_base ..................................... 500000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_interleaved .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_percent .................................. 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_scaling_factor ........................... 40[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   rotary_seq_len_interpolation_factor ............. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   router_logit_softcapping ........................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   run_workload_inspector_server ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   s3_cache_path ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sample_rate ..................................... 1.0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save ............................................ /workspace/Primus/output/amd/root/llama3.1_8B-pretrain/checkpoints[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save_interval ................................... 20000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   save_retain_interval ............................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   scatter_gather_tensors_in_pipeline .............. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   seed ............................................ 1234[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   seq_length ...................................... 8192[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sequence_parallel ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sgd_momentum .................................... 0.9[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sharp_enabled_group ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   short_seq_prob .................................. 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   sink_level ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   skip_train ...................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   skipped_train_samples ........................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   spec ............................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   split ........................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   squared_relu .................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   standalone_embedding_stage ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   start_weight_decay .............................. 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   stderr_sink_level ............................... DEBUG[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   straggler_ctrlr_port ............................ 65535[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   straggler_minmax_count .......................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   suggested_communication_unit_size ............... 400000000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   swiglu .......................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   swin_backbone_type .............................. tiny[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   te_rng_tracker .................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensor_model_parallel_size ...................... 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_dir ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_log_interval ........................ 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tensorboard_queue_size .......................... 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   test_data_path .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   test_mode ....................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_num_special_tokens ..................... 1000[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_pattern ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tiktoken_special_tokens ......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   timing_log_level ................................ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   timing_log_option ............................... minmax[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   titles_data_path ................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tokenizer_model ................................. meta-llama/Llama-3.1-8B[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tokenizer_type .................................. Llama3Tokenizer[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_record_shapes .................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_use_gzip ......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   torch_profiler_with_stack ....................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bootstrap_backend ....................... nccl[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bulk_dgrad .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_bulk_wgrad .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_ag .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_cfg ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_rs .............................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_overlap_rs_dgrad ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_split_ag ................................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   tp_comm_split_rs ................................ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_data_path ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_iters ..................................... 50[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_samples ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   train_sync_interval ............................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   trainable ....................................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   transformer_impl ................................ transformer_engine[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   transformer_pipeline_model_parallel_size ........ 1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   trust_remote_code ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_deepep_num_cu ............................. 32[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_deepep_use_comm_stream .................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   turbo_sync_free_moe_stage ....................... 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   untie_embeddings_and_output_weights ............. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_checkpoint_args ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_checkpoint_opt_param_scheduler .............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_cpu_initialization .......................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_custom_fsdp ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_deprecated_20241209_moe_layer ............... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_dist_ckpt ................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_dist_ckpt_deprecated ........................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_distributed_optimizer ....................... True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_flash_attn .................................. True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_legacy_models ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_megatron_fsdp ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_mp_args_from_checkpoint_args ................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_one_sent_docs ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_persistent_ckpt_worker ...................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_precision_aware_optimizer ................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_pytorch_profiler ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_ring_exchange_p2p ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rocm_mem_info ............................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rocm_mem_info_iters ......................... [1, 2][0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rope_scaling ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_rotary_position_embeddings .................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_sharp ....................................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_te_activation_func .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_tokenizer_model_from_checkpoint_args ........ True[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_torch_fsdp2 ................................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_torch_optimizer_for_cpu_offload ............. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_tp_pp_dp_mapping ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_attention ............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_deepep ................................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_fused_act_with_probs .................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_grouped_mlp ........................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_parallel_linear ....................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   use_turbo_rms_norm .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   v_head_dim ...................................... 128[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   valid_data_path ................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   variable_seq_lengths ............................ False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   virtual_pipeline_model_parallel_size ............ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_backbone_type ............................ vit[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_pretraining .............................. False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vision_pretraining_type ......................... classify[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_extra_ids ................................. 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_file ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   vocab_size ...................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_entity .................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_exp_name .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_project ................................... None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wandb_save_dir .................................. None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   weight_decay .................................... 0.1[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   weight_decay_incr_style ......................... constant[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   wgrad_deferral_limit ............................ 0[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   world_size ...................................... 8[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   yaml_cfg ........................................ None[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_adaptive_memory_limit_percentile .... 85[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_max_pending_backward ................ auto[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_pipeline_timers_end_iter ............ 110[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_pipeline_timers_start_iter .......... 100[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_v_schedule .......................... False[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1198] :   zero_bubble_v_schedule_mem_setup ................ half[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1199] : -------------------- end of arguments ---------------------[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1185] : -monkey patch megatron.training.global_vars._set_wandb_writer...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1190] : -set_global_variables...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1192] : -set_primus_global_variables...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1197] : -build_tokenizer...[0m
[[32m20251218 08:14:38[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------tokenizer.py:40] : -building Llama3Tokenizer tokenizer...[0m
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[--------tokenizer.py:63] :  > padded vocab (size: 128256) with 0 dummy tokens (new size: 128256)[0m
RerunStateMachine initialized in mode disabled
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1250] : -lazy_mpu_init:     None[0m
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1227] : -initialize_distributed...[0m
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:329] : > initializing torch distributed ...[0m
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 7 is connected to 67 is connected to  peer ranks. Expected number of connected peer ranks is : 77 peer ranks. Expected number of connected peer ranks is : 7

[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 76 is connected to  is connected to 77 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 77

[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1[Gloo] Rank  is connected to 37 peer ranks.  is connected to Expected number of connected peer ranks is : 77 peer ranks. 
Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 77
 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:380] : > initialized tensor model parallel with size 1[0m
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:384] : > initialized pipeline model parallel with size 1[0m
[[32m20251218 08:14:39[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1231] : -seeds:             1234[0m
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:761] : time to initialize megatron (seconds): 17.733[0m
/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after megatron is initialized] datetime: 2025-12-18 08:14:55 [0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[---------trainer.py:994] : -setup_model_and_optimizer...[0m
[[32m20251218 08:14:55[0m][[36mrank-6/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-5/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[trainer.py:243]: MegatronTrainer: patch TESpecProvider to PrimusTurboSpecProvider, `enable_primus_turbo=True` will use PrimusTurbo backend[0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1299] : use pt backend...[0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1307] : -run get_model[0m
[[32m20251218 08:14:55[0m][[36mrank-1/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-4/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-3/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-2/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[---------pretrain.py:34] : [PrimusPatch] Overriding build_model to disable second DDP construction...[0m
[[32m20251218 08:14:55[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : building GPT model ...[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[--------training.py:903] :  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 8030261248[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1309] : [DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-31): 32 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear(in_features=4096, out_features=4096, bias=False, TP=1)
              (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=6144, bias=False, TP=1)
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): IdentityOp()
            (mlp): MLP(
              (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=28672, bias=False, TP=1)
              (linear_fc2): TERowParallelLinear(in_features=14336, out_features=4096, bias=False, TP=1)
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear(in_features=4096, out_features=128256, bias=False, TP=1)
    )
  )
)][0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1323] : -run get_megatron_optimizer[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after model, optimizer, and learning rate scheduler are built] datetime: 2025-12-18 08:14:56 [0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : > building train, validation, and test datasets ...[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :  > datasets target sizes (minimum size):[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     train:      12800[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     validation: 0[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] :     test:       0[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1111] : > building train, validation, and test datasets for GPT ...[0m
Unable to save MockGPTDataset indexes because path_to_cache is None
Unable to save MockGPTDataset indexes because path_to_cache is None
Unable to save MockGPTDataset indexes because path_to_cache is None
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1119] : > finished creating GPT datasets ...[0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after dataloaders are built] datetime: 2025-12-18 08:14:56 [0m
[[32m20251218 08:14:56[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1058] : done with setup ...[0m
[[32m20251218 08:14:57[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[----------timers.py:430] : (min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1106.25, 1106.43)
    train/valid/test-data-iterators-setup ..........: (9.50, 10.29)[0m
[[32m20251218 08:14:57[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1453] : training ...[0m
[[32m20251218 08:14:57[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[--------trainer.py:1597] : Setting rerun_state_machine.current_iteration to 0...[0m
[[32m20251218 08:14:57[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-----import_utils.py:30] : [Primus][MegatronCompat] Loaded FullyShardedDataParallel from megatron.core.distributed.fsdp.mcore_fsdp_adapter[0m
[[32m20251218 08:14:57[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [before the start of training step] datetime: 2025-12-18 08:14:57 [0m
[[32m20251218 08:15:02[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[fp8_utils.py:156]: WARNING: Primus-Turbo FP8 delayed not work since Primus-Turbo not support delayed scaling..[0m
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

[[32m20251218 08:15:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        1/      50 | consumed samples:          256 | elapsed time per iteration (ms): 53350.2/53350.2 | hip mem usage/free/total/usage_ratio: 152.14GB/39.85GB/191.98GB/79.24% | rocm mem usage/free/total/usage_ratio: 152.34GB/39.64GB/191.98GB/79.35% | throughput per GPU (TFLOP/s/GPU): 284.6/284.6 | tokens per GPU (tokens/s/GPU): 4913.6/4913.6 | learning rate: 5.000000E-06 | global batch size:   256 | lm loss: 1.189806E+01 | loss scale: 1.0 | grad norm: 2.442 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:137] : Number of parameters in transformer block in billions:  6.98[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:146] : Number of parameters in embedding layers in billions: 1.05[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:150] : Total number of parameters in billions: 8.03[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:163] : Number of parameters in most loaded shard in billions: 8.0305[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : compute_activation_memory_without_sp[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:270] : Activation memory footprint per transformer layer (precise, without SP): 2176.0 MB[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:362] : Theoretical memory footprints: weight and optimizer=57438.81 MB, activation=77523.73 MB, total=134962.54 MB
[0m
[[32m20251218 08:15:50[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:275] : [Rank 0] (after 1 iterations) memory (MB) | allocated: 70942.72021484375 | max allocated: 136177.58935546875 | reserved: 145672.0 | max reserved: 145672.0[0m
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

/opt/venv/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

WARNING: AMD GPU device(s) is/are in a low-power state. Check power control/runtime_status

[[32m20251218 08:16:13[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        2/      50 | consumed samples:          512 | elapsed time per iteration (ms): 22465.6/37907.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | rocm mem usage/free/total/usage_ratio: 160.17GB/31.81GB/191.98GB/83.43% | throughput per GPU (TFLOP/s/GPU): 675.8/480.2 | tokens per GPU (tokens/s/GPU): 11668.7/8291.2 | learning rate: 1.000000E-05 | global batch size:   256 | lm loss: 1.189686E+01 | loss scale: 1.0 | grad norm: 2.755 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:16:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        3/      50 | consumed samples:          768 | elapsed time per iteration (ms): 22445.6/22445.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 676.4/676.4 | tokens per GPU (tokens/s/GPU): 11679.1/11679.1 | learning rate: 9.989295E-06 | global batch size:   256 | lm loss: 1.153998E+01 | loss scale: 1.0 | grad norm: 2.990 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:16:58[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        4/      50 | consumed samples:         1024 | elapsed time per iteration (ms): 22506.8/22476.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.5/675.5 | tokens per GPU (tokens/s/GPU): 11647.3/11663.2 | learning rate: 9.957224E-06 | global batch size:   256 | lm loss: 1.096591E+01 | loss scale: 1.0 | grad norm: 3.484 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:17:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        5/      50 | consumed samples:         1280 | elapsed time per iteration (ms): 22509.7/22487.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.4/675.1 | tokens per GPU (tokens/s/GPU): 11645.8/11657.4 | learning rate: 9.903926E-06 | global batch size:   256 | lm loss: 1.055143E+01 | loss scale: 1.0 | grad norm: 4.643 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:17:43[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        6/      50 | consumed samples:         1536 | elapsed time per iteration (ms): 22487.2/22487.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.1/675.1 | tokens per GPU (tokens/s/GPU): 11657.5/11657.4 | learning rate: 9.829630E-06 | global batch size:   256 | lm loss: 1.019356E+01 | loss scale: 1.0 | grad norm: 6.822 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:18:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        7/      50 | consumed samples:         1792 | elapsed time per iteration (ms): 22461.9/22482.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.9/675.3 | tokens per GPU (tokens/s/GPU): 11670.6/11660.1 | learning rate: 9.734651E-06 | global batch size:   256 | lm loss: 9.687737E+00 | loss scale: 1.0 | grad norm: 9.082 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:18:28[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        8/      50 | consumed samples:         2048 | elapsed time per iteration (ms): 22473.3/22480.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.3 | tokens per GPU (tokens/s/GPU): 11664.7/11660.8 | learning rate: 9.619398E-06 | global batch size:   256 | lm loss: 9.164751E+00 | loss scale: 1.0 | grad norm: 9.455 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:18:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration        9/      50 | consumed samples:         2304 | elapsed time per iteration (ms): 22472.6/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.3 | tokens per GPU (tokens/s/GPU): 11665.1/11661.4 | learning rate: 9.484364E-06 | global batch size:   256 | lm loss: 8.593085E+00 | loss scale: 1.0 | grad norm: 8.906 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:19:13[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       10/      50 | consumed samples:         2560 | elapsed time per iteration (ms): 22478.5/22479.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.4 | tokens per GPU (tokens/s/GPU): 11662.0/11661.5 | learning rate: 9.330127E-06 | global batch size:   256 | lm loss: 8.169315E+00 | loss scale: 1.0 | grad norm: 8.239 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:19:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       11/      50 | consumed samples:         2816 | elapsed time per iteration (ms): 22479.8/22479.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.4 | tokens per GPU (tokens/s/GPU): 11661.3/11661.5 | learning rate: 9.157348E-06 | global batch size:   256 | lm loss: 7.780855E+00 | loss scale: 1.0 | grad norm: 8.525 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:19:58[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       12/      50 | consumed samples:         3072 | elapsed time per iteration (ms): 22480.5/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11661.0/11661.4 | learning rate: 8.966766E-06 | global batch size:   256 | lm loss: 7.481033E+00 | loss scale: 1.0 | grad norm: 8.694 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:20:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       13/      50 | consumed samples:         3328 | elapsed time per iteration (ms): 22482.7/22479.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11659.8/11661.3 | learning rate: 8.759199E-06 | global batch size:   256 | lm loss: 7.169314E+00 | loss scale: 1.0 | grad norm: 10.056 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:20:42[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       14/      50 | consumed samples:         3584 | elapsed time per iteration (ms): 22474.0/22479.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.4 | tokens per GPU (tokens/s/GPU): 11664.3/11661.5 | learning rate: 8.535534E-06 | global batch size:   256 | lm loss: 6.815296E+00 | loss scale: 1.0 | grad norm: 10.386 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:21:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       15/      50 | consumed samples:         3840 | elapsed time per iteration (ms): 22482.0/22479.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11660.2/11661.4 | learning rate: 8.296729E-06 | global batch size:   256 | lm loss: 6.555819E+00 | loss scale: 1.0 | grad norm: 11.956 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:21:27[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       16/      50 | consumed samples:         4096 | elapsed time per iteration (ms): 22481.4/22479.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.3/675.3 | tokens per GPU (tokens/s/GPU): 11660.5/11661.4 | learning rate: 8.043807E-06 | global batch size:   256 | lm loss: 6.376563E+00 | loss scale: 1.0 | grad norm: 13.170 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:21:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       17/      50 | consumed samples:         4352 | elapsed time per iteration (ms): 22470.7/22479.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.4 | tokens per GPU (tokens/s/GPU): 11666.0/11661.7 | learning rate: 7.777851E-06 | global batch size:   256 | lm loss: 6.151232E+00 | loss scale: 1.0 | grad norm: 14.342 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:22:12[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       18/      50 | consumed samples:         4608 | elapsed time per iteration (ms): 22477.3/22479.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.4 | tokens per GPU (tokens/s/GPU): 11662.6/11661.7 | learning rate: 7.500000E-06 | global batch size:   256 | lm loss: 6.045423E+00 | loss scale: 1.0 | grad norm: 15.889 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:22:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       19/      50 | consumed samples:         4864 | elapsed time per iteration (ms): 22495.0/22479.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.9/675.3 | tokens per GPU (tokens/s/GPU): 11653.4/11661.2 | learning rate: 7.211444E-06 | global batch size:   256 | lm loss: 5.881318E+00 | loss scale: 1.0 | grad norm: 15.935 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:22:57[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       20/      50 | consumed samples:         5120 | elapsed time per iteration (ms): 22483.9/22480.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.3 | tokens per GPU (tokens/s/GPU): 11659.2/11661.1 | learning rate: 6.913417E-06 | global batch size:   256 | lm loss: 5.755231E+00 | loss scale: 1.0 | grad norm: 18.512 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:23:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       21/      50 | consumed samples:         5376 | elapsed time per iteration (ms): 22483.5/22480.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.3 | tokens per GPU (tokens/s/GPU): 11659.4/11661.0 | learning rate: 6.607198E-06 | global batch size:   256 | lm loss: 5.650851E+00 | loss scale: 1.0 | grad norm: 18.909 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:23:42[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       22/      50 | consumed samples:         5632 | elapsed time per iteration (ms): 22477.2/22480.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.3 | tokens per GPU (tokens/s/GPU): 11662.7/11661.1 | learning rate: 6.294095E-06 | global batch size:   256 | lm loss: 5.612560E+00 | loss scale: 1.0 | grad norm: 20.517 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:24:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       23/      50 | consumed samples:         5888 | elapsed time per iteration (ms): 22470.3/22479.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/675.3 | tokens per GPU (tokens/s/GPU): 11666.2/11661.4 | learning rate: 5.975452E-06 | global batch size:   256 | lm loss: 5.469757E+00 | loss scale: 1.0 | grad norm: 22.554 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:24:27[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       24/      50 | consumed samples:         6144 | elapsed time per iteration (ms): 22464.8/22479.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.8/675.4 | tokens per GPU (tokens/s/GPU): 11669.1/11661.7 | learning rate: 5.652631E-06 | global batch size:   256 | lm loss: 5.307073E+00 | loss scale: 1.0 | grad norm: 24.546 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:24:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       25/      50 | consumed samples:         6400 | elapsed time per iteration (ms): 22447.6/22477.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 676.3/675.4 | tokens per GPU (tokens/s/GPU): 11678.1/11662.4 | learning rate: 5.327016E-06 | global batch size:   256 | lm loss: 5.141537E+00 | loss scale: 1.0 | grad norm: 26.353 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:25:12[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       26/      50 | consumed samples:         6656 | elapsed time per iteration (ms): 22474.9/22477.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.4 | tokens per GPU (tokens/s/GPU): 11663.9/11662.5 | learning rate: 5.000000E-06 | global batch size:   256 | lm loss: 5.054605E+00 | loss scale: 1.0 | grad norm: 26.109 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:25:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       27/      50 | consumed samples:         6912 | elapsed time per iteration (ms): 23003.1/22498.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 660.0/674.8 | tokens per GPU (tokens/s/GPU): 11396.0/11651.8 | learning rate: 4.672984E-06 | global batch size:   256 | lm loss: 4.834011E+00 | loss scale: 1.0 | grad norm: 27.043 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:25:58[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       28/      50 | consumed samples:         7168 | elapsed time per iteration (ms): 22471.9/22497.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/674.8 | tokens per GPU (tokens/s/GPU): 11665.4/11652.4 | learning rate: 4.347369E-06 | global batch size:   256 | lm loss: 4.673080E+00 | loss scale: 1.0 | grad norm: 27.981 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:26:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       29/      50 | consumed samples:         7424 | elapsed time per iteration (ms): 22468.6/22496.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.7/674.9 | tokens per GPU (tokens/s/GPU): 11667.1/11652.9 | learning rate: 4.024549E-06 | global batch size:   256 | lm loss: 4.520215E+00 | loss scale: 1.0 | grad norm: 30.762 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:26:43[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       30/      50 | consumed samples:         7680 | elapsed time per iteration (ms): 22463.3/22495.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.8/674.9 | tokens per GPU (tokens/s/GPU): 11669.9/11653.5 | learning rate: 3.705905E-06 | global batch size:   256 | lm loss: 4.383165E+00 | loss scale: 1.0 | grad norm: 29.480 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:27:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       31/      50 | consumed samples:         7936 | elapsed time per iteration (ms): 22470.2/22494.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.6/674.9 | tokens per GPU (tokens/s/GPU): 11666.3/11653.9 | learning rate: 3.392803E-06 | global batch size:   256 | lm loss: 4.145537E+00 | loss scale: 1.0 | grad norm: 30.584 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:27:28[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       32/      50 | consumed samples:         8192 | elapsed time per iteration (ms): 22467.0/22493.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.7/674.9 | tokens per GPU (tokens/s/GPU): 11667.9/11654.4 | learning rate: 3.086583E-06 | global batch size:   256 | lm loss: 4.018104E+00 | loss scale: 1.0 | grad norm: 28.627 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:27:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       33/      50 | consumed samples:         8448 | elapsed time per iteration (ms): 22472.8/22492.8 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.5/675.0 | tokens per GPU (tokens/s/GPU): 11664.9/11654.8 | learning rate: 2.788556E-06 | global batch size:   256 | lm loss: 3.885016E+00 | loss scale: 1.0 | grad norm: 27.179 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:28:12[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       34/      50 | consumed samples:         8704 | elapsed time per iteration (ms): 22478.6/22492.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.4/675.0 | tokens per GPU (tokens/s/GPU): 11661.9/11655.0 | learning rate: 2.500000E-06 | global batch size:   256 | lm loss: 3.726072E+00 | loss scale: 1.0 | grad norm: 23.796 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:28:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       35/      50 | consumed samples:         8960 | elapsed time per iteration (ms): 22484.9/22492.2 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 675.2/675.0 | tokens per GPU (tokens/s/GPU): 11658.7/11655.1 | learning rate: 2.222149E-06 | global batch size:   256 | lm loss: 3.581526E+00 | loss scale: 1.0 | grad norm: 23.021 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:28:57[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       36/      50 | consumed samples:         9216 | elapsed time per iteration (ms): 22500.3/22492.4 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.7/675.0 | tokens per GPU (tokens/s/GPU): 11650.7/11655.0 | learning rate: 1.956193E-06 | global batch size:   256 | lm loss: 3.492316E+00 | loss scale: 1.0 | grad norm: 21.526 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:29:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       37/      50 | consumed samples:         9472 | elapsed time per iteration (ms): 22499.4/22492.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.8/675.0 | tokens per GPU (tokens/s/GPU): 11651.1/11654.8 | learning rate: 1.703271E-06 | global batch size:   256 | lm loss: 3.318976E+00 | loss scale: 1.0 | grad norm: 20.294 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:29:42[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       38/      50 | consumed samples:         9728 | elapsed time per iteration (ms): 22505.8/22493.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.6/675.0 | tokens per GPU (tokens/s/GPU): 11647.9/11654.7 | learning rate: 1.464466E-06 | global batch size:   256 | lm loss: 3.240969E+00 | loss scale: 1.0 | grad norm: 19.176 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:30:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       39/      50 | consumed samples:         9984 | elapsed time per iteration (ms): 22498.1/22493.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.8/674.9 | tokens per GPU (tokens/s/GPU): 11651.8/11654.6 | learning rate: 1.240801E-06 | global batch size:   256 | lm loss: 3.152150E+00 | loss scale: 1.0 | grad norm: 17.439 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:30:27[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       40/      50 | consumed samples:        10240 | elapsed time per iteration (ms): 22500.3/22493.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.7/674.9 | tokens per GPU (tokens/s/GPU): 11650.7/11654.5 | learning rate: 1.033233E-06 | global batch size:   256 | lm loss: 2.999306E+00 | loss scale: 1.0 | grad norm: 16.687 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:30:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       41/      50 | consumed samples:        10496 | elapsed time per iteration (ms): 22516.0/22493.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.3/674.9 | tokens per GPU (tokens/s/GPU): 11642.6/11654.2 | learning rate: 8.426520E-07 | global batch size:   256 | lm loss: 3.024853E+00 | loss scale: 1.0 | grad norm: 16.829 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:31:13[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       42/      50 | consumed samples:        10752 | elapsed time per iteration (ms): 22523.2/22494.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.9 | tokens per GPU (tokens/s/GPU): 11638.8/11653.8 | learning rate: 6.698730E-07 | global batch size:   256 | lm loss: 2.928720E+00 | loss scale: 1.0 | grad norm: 16.137 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:31:35[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       43/      50 | consumed samples:        11008 | elapsed time per iteration (ms): 22533.5/22495.6 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.9 | tokens per GPU (tokens/s/GPU): 11633.5/11653.3 | learning rate: 5.156363E-07 | global batch size:   256 | lm loss: 2.889323E+00 | loss scale: 1.0 | grad norm: 14.716 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:31:58[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       44/      50 | consumed samples:        11264 | elapsed time per iteration (ms): 22524.9/22496.3 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.9 | tokens per GPU (tokens/s/GPU): 11638.0/11652.9 | learning rate: 3.806023E-07 | global batch size:   256 | lm loss: 2.868090E+00 | loss scale: 1.0 | grad norm: 13.824 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:32:20[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       45/      50 | consumed samples:        11520 | elapsed time per iteration (ms): 22531.7/22497.1 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.8/674.8 | tokens per GPU (tokens/s/GPU): 11634.5/11652.5 | learning rate: 2.653493E-07 | global batch size:   256 | lm loss: 2.806202E+00 | loss scale: 1.0 | grad norm: 12.779 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:32:43[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       46/      50 | consumed samples:        11776 | elapsed time per iteration (ms): 22534.8/22497.9 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11632.8/11652.1 | learning rate: 1.703709E-07 | global batch size:   256 | lm loss: 2.790954E+00 | loss scale: 1.0 | grad norm: 12.843 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:33:05[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       47/      50 | consumed samples:        12032 | elapsed time per iteration (ms): 22533.1/22498.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11633.7/11651.6 | learning rate: 9.607360E-08 | global batch size:   256 | lm loss: 2.741159E+00 | loss scale: 1.0 | grad norm: 12.762 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:33:28[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       48/      50 | consumed samples:        12288 | elapsed time per iteration (ms): 22534.5/22499.5 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.7/674.8 | tokens per GPU (tokens/s/GPU): 11633.0/11651.2 | learning rate: 4.277569E-08 | global batch size:   256 | lm loss: 2.793205E+00 | loss scale: 1.0 | grad norm: 11.433 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:33:50[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       49/      50 | consumed samples:        12544 | elapsed time per iteration (ms): 22524.3/22500.0 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 674.0/674.7 | tokens per GPU (tokens/s/GPU): 11638.3/11651.0 | learning rate: 1.070538E-08 | global batch size:   256 | lm loss: 2.646500E+00 | loss scale: 1.0 | grad norm: 12.222 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:34:13[0m][[36mrank-7/8[0m][[1mINFO [0m] [1m[--------trainer.py:2578] :  iteration       50/      50 | consumed samples:        12800 | elapsed time per iteration (ms): 22532.0/22500.7 | hip mem usage/free/total/usage_ratio: 159.96GB/32.02GB/191.98GB/83.32% | throughput per GPU (TFLOP/s/GPU): 673.8/674.7 | tokens per GPU (tokens/s/GPU): 11634.3/11650.6 | learning rate: 0.000000E+00 | global batch size:   256 | lm loss: 2.718093E+00 | loss scale: 1.0 | grad norm: 11.622 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20251218 08:34:13[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:385] : [after training is done] datetime: 2025-12-18 08:34:13 [0m
