# Week 02 - GPU Benchmarking System Index

## ğŸ“ You Are Here

```
support/
â”œâ”€â”€ PROFILING_SUMMARY.md          â† ğŸ“– Complete overview (start here)
â””â”€â”€ week-02/
    â”œâ”€â”€ INDEX.md                  â† ğŸ“ You are here
    â”œâ”€â”€ BENCHMARKING_GUIDE.md     â† ğŸ“š High-level guide
    â”‚
    â”œâ”€â”€ code/                     â† ğŸ’» All code and scripts
    â”‚   â”œâ”€â”€ README.md             â† Quick index
    â”‚   â”œâ”€â”€ QUICK_START.md        â† 5-min guide
    â”‚   â”œâ”€â”€ WORKFLOW.md           â† Visual diagrams
    â”‚   â”œâ”€â”€ BENCHMARK_README.md   â† Complete reference
    â”‚   â”‚
    â”‚   â”œâ”€â”€ benchmark_utils.py    â† â­ Core framework
    â”‚   â”œâ”€â”€ compare_results.py    â† â­ Comparison tool
    â”‚   â”œâ”€â”€ analyze_existing_logs.py
    â”‚   â”œâ”€â”€ run_benchmark.sh      â† â­ Automation script
    â”‚   â”‚
    â”‚   â”œâ”€â”€ pretrain_llama.py     â† Updated with benchmarking
    â”‚   â”œâ”€â”€ pretrain_qwen.py      â† Updated with benchmarking
    â”‚   â”œâ”€â”€ pretrain_mistral.py   â† Updated with benchmarking
    â”‚   â”‚
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â””â”€â”€ benchmark_results/    â† Generated results go here
    â”‚
    â”œâ”€â”€ amd-logs/                 â† Your existing AMD profiling
    â”‚   â”œâ”€â”€ llama/                (8 Excel files)
    â”‚   â””â”€â”€ qwen/                 (8 Excel files)
    â”‚
    â””â”€â”€ nvi-logs/                 â† Your existing NVIDIA profiling
        â”œâ”€â”€ llama3_1_8b_pretrain_fp8/  (TensorBoard events)
        â””â”€â”€ qwen25_7b_test_fp8/        (TensorBoard events)
```

## ğŸ¯ What to Read Based on Your Goal

### Goal: Just Run It (5 minutes)
```
1. Read: code/QUICK_START.md
2. Run:  ./run_benchmark.sh llama
3. Done!
```

### Goal: Understand How It Works (15 minutes)
```
1. Read: code/WORKFLOW.md        (visual diagrams)
2. Read: BENCHMARKING_GUIDE.md   (overview)
3. Explore: code/benchmark_utils.py
```

### Goal: Deep Dive (30 minutes)
```
1. Read: code/BENCHMARK_README.md  (complete reference)
2. Read: code/benchmark_utils.py   (implementation)
3. Read: code/compare_results.py   (analysis)
```

### Goal: Quick Reference
```
Keep open: code/README.md (command reference)
```

## ğŸ“š Documentation Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENTATION TREE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“– PROFILING_SUMMARY.md (../PROFILING_SUMMARY.md)
   â”‚
   â”œâ”€ What was created
   â”œâ”€ Problem solved
   â”œâ”€ File locations
   â””â”€ Quick reference

ğŸ“š BENCHMARKING_GUIDE.md (./BENCHMARKING_GUIDE.md)
   â”‚
   â”œâ”€ Overview and features
   â”œâ”€ Quick start
   â”œâ”€ Hardware configuration
   â”œâ”€ Performance metrics
   â”œâ”€ Usage scenarios
   â”œâ”€ Example output
   â”œâ”€ Configuration details
   â”œâ”€ Troubleshooting
   â””â”€ Best practices

ğŸ“ INDEX.md (./INDEX.md) â† YOU ARE HERE
   â”‚
   â”œâ”€ File structure
   â”œâ”€ Documentation map
   â””â”€ Quick navigation

code/
â”‚
â”œâ”€ ğŸ“– README.md
â”‚  â”‚
â”‚  â”œâ”€ Quick overview
â”‚  â”œâ”€ What's included
â”‚  â”œâ”€ What you get
â”‚  â”œâ”€ Key features
â”‚  â”œâ”€ Usage examples
â”‚  â”œâ”€ Common issues
â”‚  â””â”€ Quick help table
â”‚
â”œâ”€ ğŸš€ QUICK_START.md
â”‚  â”‚
â”‚  â”œâ”€ TL;DR (3 commands)
â”‚  â”œâ”€ Step-by-step guide
â”‚  â”œâ”€ Available models
â”‚  â”œâ”€ Troubleshooting
â”‚  â””â”€ Example results
â”‚
â”œâ”€ ğŸ“Š WORKFLOW.md
â”‚  â”‚
â”‚  â”œâ”€ Visual workflow diagram
â”‚  â”œâ”€ Component architecture
â”‚  â”œâ”€ Data flow diagrams
â”‚  â”œâ”€ File interactions
â”‚  â”œâ”€ Metrics timeline
â”‚  â””â”€ Output structure
â”‚
â””â”€ ğŸ“š BENCHMARK_README.md
   â”‚
   â”œâ”€ Detailed overview
   â”œâ”€ Configuration guide
   â”œâ”€ Metrics explanation
   â”œâ”€ Understanding results
   â”œâ”€ Advanced profiling
   â”œâ”€ Best practices
   â””â”€ Complete reference
```

## ğŸ’» Code Files

### Core Framework (Must Read)

| File | Size | Purpose |
|------|------|---------|
| `benchmark_utils.py` | 10KB | Platform-agnostic benchmarking framework |
| `compare_results.py` | 11KB | Generate comparison reports and charts |
| `run_benchmark.sh` | 3.5KB | Automated benchmark runner |

### Training Scripts (Updated)

| File | Size | Model | Status |
|------|------|-------|--------|
| `pretrain_llama.py` | 1.5KB | Llama 3.1 8B | âœ… Updated |
| `pretrain_qwen.py` | 1.5KB | Qwen 2.5 7B | âœ… Updated |
| `pretrain_mistral.py` | 1.5KB | Mistral 7B | âœ… Updated |

### Utilities

| File | Size | Purpose |
|------|------|---------|
| `analyze_existing_logs.py` | 7KB | Analyze old AMD/NVIDIA logs |
| `requirements.txt` | 212B | Python dependencies |

### Conversion Scripts (Unchanged)

| File | Purpose |
|------|---------|
| `convert_llama.py` | Convert Llama checkpoints |
| `convert_qwen.py` | Convert Qwen checkpoints |
| `convert_mistral.py` | Convert Mistral checkpoints |

## ğŸš€ Quick Commands

### Essential Commands

```bash
# Navigate to code directory
cd week-02/code

# Run benchmark (auto-detects platform)
./run_benchmark.sh llama

# Compare results (after running on both platforms)
python3 compare_results.py

# Check existing logs
python3 analyze_existing_logs.py
```

### Advanced Commands

```bash
# Multiple runs for statistical significance
./run_benchmark.sh llama 5

# Run all models
for model in llama qwen mistral; do
    ./run_benchmark.sh $model
done

# Check results directory
ls -lh benchmark_results/

# View latest result
cat benchmark_results/benchmark_*.json | tail -1 | python3 -m json.tool
```

## ğŸ“Š What Gets Generated

### During Training

```
Terminal Output:
[CUDA] Step  10 | Time: 1.234s | Avg: 1.245s | Memory: 45.67GB
[CUDA] Step  20 | Time: 1.238s | Avg: 1.242s | Memory: 45.68GB
```

### After Training

```
benchmark_results/
â””â”€â”€ benchmark_cuda_20260105_143022.json
    â”œâ”€â”€ platform: "cuda"
    â”œâ”€â”€ gpu_info: {...}
    â”œâ”€â”€ training_config: {...}
    â”œâ”€â”€ performance_metrics: {...}
    â”œâ”€â”€ memory_metrics: {...}
    â””â”€â”€ raw_step_times: [...]
```

### After Comparison

```
benchmark_results/
â”œâ”€â”€ comparison_plot.png          â† 4-panel visualization
â”‚   â”œâ”€â”€ Average Step Time (bar chart)
â”‚   â”œâ”€â”€ Throughput (bar chart)
â”‚   â”œâ”€â”€ Memory Usage (grouped bars)
â”‚   â””â”€â”€ Step Time Distribution (line plot)
â”‚
â””â”€â”€ comparison_report.md         â† Detailed markdown report
    â”œâ”€â”€ Executive Summary
    â”œâ”€â”€ Hardware Configuration
    â”œâ”€â”€ Performance Metrics
    â”œâ”€â”€ Memory Usage
    â””â”€â”€ Detailed Analysis
```

## ğŸ¯ Common Workflows

### Workflow 1: First Time Setup

```bash
# 1. Install dependencies
pip install matplotlib numpy

# 2. Read quick start
cat code/QUICK_START.md

# 3. Run on current platform
cd code
./run_benchmark.sh llama

# 4. Check results
ls benchmark_results/
```

### Workflow 2: Full Comparison

```bash
# On NVIDIA system
cd week-02/code
./run_benchmark.sh llama

# Copy JSON to shared location or USB drive
cp benchmark_results/benchmark_cuda_*.json /path/to/shared/

# On AMD system
cd week-02/code
./run_benchmark.sh llama

# Copy both JSONs to comparison machine
# Then compare
python3 compare_results.py

# View results
open comparison_plot.png
cat comparison_report.md
```

### Workflow 3: Multi-Model Analysis

```bash
# Run all three models on both platforms
cd week-02/code

# On each platform
for model in llama qwen mistral; do
    echo "Running $model..."
    ./run_benchmark.sh $model
    sleep 30  # Cool down
done

# Compare each model
python3 compare_results.py
```

### Workflow 4: Statistical Analysis

```bash
# Run 5 times on each platform
cd week-02/code
./run_benchmark.sh llama 5

# Results will be averaged automatically
python3 compare_results.py
```

## ğŸ” Understanding the System

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Training Script                        â”‚
â”‚  (pretrain_llama.py / qwen.py / mistral.py)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”œâ”€ Imports benchmark_utils.py
                          â”œâ”€ Creates BenchmarkCallback
                          â””â”€ Adds to trainer.callbacks
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BenchmarkCallback                          â”‚
â”‚  (from benchmark_utils.py)                             â”‚
â”‚                                                         â”‚
â”‚  on_train_start()    â†’ Detect platform, get GPU info   â”‚
â”‚  on_batch_start()    â†’ Start timer, sync GPU           â”‚
â”‚  on_batch_end()      â†’ Stop timer, record metrics      â”‚
â”‚  on_train_end()      â†’ Save JSON, print summary        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              JSON Results                               â”‚
â”‚  benchmark_results/benchmark_{platform}_{time}.json    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              compare_results.py                         â”‚
â”‚                                                         â”‚
â”‚  load_benchmark_results()    â†’ Load JSONs              â”‚
â”‚  create_comparison_plot()    â†’ Generate chart          â”‚
â”‚  generate_comparison_report() â†’ Create markdown        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Final Output                               â”‚
â”‚  - comparison_plot.png                                  â”‚
â”‚  - comparison_report.md                                 â”‚
â”‚  - Console summary                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

1. **Platform Agnostic**: Auto-detects CUDA vs ROCm
2. **Non-Invasive**: Just a callback, no training changes
3. **Fair**: Identical configs, same warmup, same sync points
4. **Automated**: Scripts handle everything
5. **Comprehensive**: Multiple metrics, stats, visualizations

## ğŸ“ Learning Path

### Beginner (Just want results)
```
1. Read: code/QUICK_START.md (5 min)
2. Run:  ./run_benchmark.sh llama
3. Done: View results
```

### Intermediate (Want to understand)
```
1. Read: code/WORKFLOW.md (10 min)
2. Read: BENCHMARKING_GUIDE.md (15 min)
3. Explore: benchmark_utils.py
4. Experiment: Try different models
```

### Advanced (Want to customize)
```
1. Read: code/BENCHMARK_README.md (20 min)
2. Study: benchmark_utils.py (full code)
3. Study: compare_results.py (full code)
4. Modify: Add custom metrics
5. Extend: Add new models
```

## ğŸ“ Quick Help

| Question | Answer |
|----------|--------|
| Where do I start? | Read `PROFILING_SUMMARY.md` in parent directory |
| How do I run it? | `cd code && ./run_benchmark.sh llama` |
| Where are results? | `code/benchmark_results/` |
| How do I compare? | `cd code && python3 compare_results.py` |
| Need quick reference? | Open `code/README.md` |
| Want visual guide? | Open `code/WORKFLOW.md` |
| Need all details? | Open `code/BENCHMARK_README.md` |
| Something broke? | Check troubleshooting in any README |

## âœ… Checklist

### Before Running
- [ ] Read `code/QUICK_START.md`
- [ ] Install dependencies: `pip install matplotlib numpy`
- [ ] Verify GPU: `nvidia-smi` or `rocm-smi`
- [ ] Navigate to: `cd week-02/code`

### Running on NVIDIA
- [ ] Run: `./run_benchmark.sh llama`
- [ ] Check: `ls benchmark_results/benchmark_cuda_*.json`
- [ ] Copy JSON to shared location (if comparing on different machine)

### Running on AMD
- [ ] Run: `./run_benchmark.sh llama`
- [ ] Check: `ls benchmark_results/benchmark_rocm_*.json`
- [ ] Ensure both CUDA and ROCm JSONs are in `benchmark_results/`

### Comparing Results
- [ ] Run: `python3 compare_results.py`
- [ ] View: `comparison_plot.png`
- [ ] Read: `comparison_report.md`
- [ ] Note: Winner and speedup factor

## ğŸ‰ Summary

You have a **complete benchmarking system** with:

âœ… **4 documentation files** (Quick Start, Workflow, Complete Guide, Overview)  
âœ… **3 core tools** (benchmark_utils, compare_results, analyze_logs)  
âœ… **3 updated training scripts** (Llama, Qwen, Mistral)  
âœ… **1 automation script** (run_benchmark.sh)  
âœ… **Full automation** (3 commands to results)  

**Next**: Read `code/QUICK_START.md` and run your first benchmark!

---

**Location**: `/Users/dmitrynvm/Work/support/week-02/`  
**Status**: âœ… Ready to use  
**Updated**: January 5, 2026  

